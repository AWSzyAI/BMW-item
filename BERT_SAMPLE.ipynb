{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# 仅本地模式（无网络）\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d56f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/google-bert/bert-base-chinese'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, \n",
    "        num_labels=3,  # 假设3分类任务，根据你的实际任务调整\n",
    "        local_files_only=True,\n",
    "        ignore_mismatched_sizes=True  # 允许分类头尺寸不匹配\n",
    "    )\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda392ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data():\n",
    "    X_train = pd.read_csv('output/2025_up_to_month_2/X_train.csv')\n",
    "    y_train = pd.read_csv('output/2025_up_to_month_2/y_train.csv')\n",
    "    X_test = pd.read_csv('output/2025_up_to_month_2/X_test.csv')\n",
    "    y_test = pd.read_csv('output/2025_up_to_month_2/y_test.csv')\n",
    "    label_mapping = pd.read_csv('output/2025_up_to_month_2/label_mapping.csv')\n",
    "    \n",
    "    # 期望列：y_* 至少包含 label, linked_items；若存在 item_title 列则一并使用\n",
    "    expected_cols = ['label', 'linked_items']\n",
    "    for name, y in [('y_train', y_train), ('y_test', y_test)]:\n",
    "        for col in expected_cols:\n",
    "            if col not in y.columns:\n",
    "                raise ValueError(f\"{name} 缺少必需列: {col}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, label_mapping\n",
    "\n",
    "# 兼容原单任务数据集（保留）\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# CoT 多任务数据集：输入 case_title + performed_work，输出两种标签\n",
    "class CoTDataset(Dataset):\n",
    "    def __init__(self, X_df, labels_linked, labels_item, tokenizer, max_length=128):\n",
    "        self.texts = (\n",
    "            X_df.iloc[:, 0].astype(str) + ' [SEP] ' + X_df.iloc[:, 1].astype(str)\n",
    "        ).tolist()\n",
    "        self.labels_linked = labels_linked\n",
    "        self.labels_item = labels_item\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label_linked = int(self.labels_linked[idx])\n",
    "        label_item = int(self.labels_item[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels_linked': torch.tensor(label_linked, dtype=torch.long),\n",
    "            'labels_item': torch.tensor(label_item, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 多任务 BERT：同时预测 linked_items 与 item_title\n",
    "class MultiTaskBert(nn.Module):\n",
    "    def __init__(self, model_path, num_labels_linked, num_labels_item):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier_linked = nn.Linear(hidden, num_labels_linked)\n",
    "        self.classifier_item = nn.Linear(hidden, num_labels_item)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels_linked=None, labels_item=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls = outputs.last_hidden_state[:, 0]\n",
    "        cls = self.dropout(cls)\n",
    "        logits_linked = self.classifier_linked(cls)\n",
    "        logits_item = self.classifier_item(cls)\n",
    "\n",
    "        loss = None\n",
    "        if labels_linked is not None and labels_item is not None:\n",
    "            ce = nn.CrossEntropyLoss()\n",
    "            loss = ce(logits_linked, labels_linked) + 0.3 * ce(logits_item, labels_item)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits_linked': logits_linked,\n",
    "            'logits_item': logits_item\n",
    "        }\n",
    "\n",
    "# 训练（CoT）\n",
    "def train_model_cot(model, train_loader, val_loader, optimizer, num_epochs=5, device='cuda'):\n",
    "    model.train()\n",
    "    train_losses, val_losses, val_acc_linked = [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_linked = batch['labels_linked'].to(device)\n",
    "            labels_item = batch['labels_item'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            labels_linked=labels_linked, labels_item=labels_item)\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += float(loss.item())\n",
    "        avg_train = total_train_loss / max(len(train_loader), 1)\n",
    "        train_losses.append(avg_train)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        all_preds_linked, all_labels_linked = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels_linked = batch['labels_linked'].to(device)\n",
    "                labels_item = batch['labels_item'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                labels_linked=labels_linked, labels_item=labels_item)\n",
    "                loss = outputs['loss']\n",
    "                total_val_loss += float(loss.item())\n",
    "                preds_linked = torch.argmax(outputs['logits_linked'], dim=1)\n",
    "                all_preds_linked.extend(preds_linked.cpu().numpy())\n",
    "                all_labels_linked.extend(labels_linked.cpu().numpy())\n",
    "        avg_val = total_val_loss / max(len(val_loader), 1)\n",
    "        val_losses.append(avg_val)\n",
    "        val_acc = accuracy_score(all_labels_linked, all_preds_linked) if all_labels_linked else 0.0\n",
    "        val_acc_linked.append(val_acc)\n",
    "        print(f'Epoch {epoch+1}: Train {avg_train:.4f} | Val {avg_val:.4f} | ValAcc(linked) {val_acc:.4f}')\n",
    "    return train_losses, val_losses, val_acc_linked\n",
    "\n",
    "# 测试（主任务）\n",
    "def evaluate_model_cot(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds_linked, all_labels_linked = [], []\n",
    "    all_preds_item, all_labels_item = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_linked = batch['labels_linked'].to(device)\n",
    "            labels_item = batch['labels_item'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            labels_linked=labels_linked, labels_item=labels_item)\n",
    "            preds_linked = torch.argmax(outputs['logits_linked'], dim=1)\n",
    "            preds_item = torch.argmax(outputs['logits_item'], dim=1)\n",
    "            all_preds_linked.extend(preds_linked.cpu().numpy())\n",
    "            all_labels_linked.extend(labels_linked.cpu().numpy())\n",
    "            all_preds_item.extend(preds_item.cpu().numpy())\n",
    "            all_labels_item.extend(labels_item.cpu().numpy())\n",
    "    acc_linked = accuracy_score(all_labels_linked, all_preds_linked) if all_labels_linked else 0.0\n",
    "    acc_item = accuracy_score(all_labels_item, all_preds_item) if all_labels_item else 0.0\n",
    "    print(f'Test Accuracy (linked_items): {acc_linked:.4f}')\n",
    "    print(f'Test Accuracy (item_title):  {acc_item:.4f}')\n",
    "    return acc_linked, acc_item, all_preds_linked, all_labels_linked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train data shape: (6244, 3)\n",
      "Train labels shape: (6244, 1)\n",
      "Test data shape: (7670, 3)\n",
      "Test labels shape: (7670, 1)\n",
      "Number of unique labels: 1161\n",
      "\n",
      "Sample training data:\n",
      "                                          case_title  \\\n",
      "0                                           制动系统故障报警   \n",
      "1                                            中央显示屏黑屏   \n",
      "2                                   购买远程温控后车机上没有安装软件   \n",
      "3  [CIC][My Car][Vehicle status][PaCC]使用问题-保养提醒/报...   \n",
      "4                                               底盘异响   \n",
      "\n",
      "                                      performed_work    month  \n",
      "0  1. 车辆显示制动系统报警，客户刚提的新车， 电脑测试无相关故障代码\\n2.检查车辆版本也是...  2025-01  \n",
      "1  1车辆进店检查中央显示屏黑屏2检查车辆未发现加装 改装 刷隐藏3此车在其他店断电 强制重启操...  2025-01  \n",
      "2    客户12月22号购买远程温控，车机端没有推送软件。我们尝试编程，给车辆充电。依旧没有安装...  2025-01  \n",
      "3  1.客户表示12月8日APP提示有传动系统检查提示，有更换过燃油滤芯，12.9日到山东临沂通...  2025-01  \n",
      "4  陪同客户试车确认异响存在车辆在低速转向及倒车转向时有异响存在，\\n举升车辆检查发现两前横向摆...  2025-01  \n",
      "\n",
      "Sample training labels:\n",
      "   linked_items\n",
      "0           409\n",
      "1           647\n",
      "2          1030\n",
      "3           797\n",
      "4           901\n",
      "\n",
      "Sample texts: ['制动系统故障报警', '中央显示屏黑屏', '购买远程温控后车机上没有安装软件']\n",
      "Sample labels: [409, 647, 1030]\n",
      "Using device: cpu\n",
      "Loading local model from: ./models/google-bert/bert-base-chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./models/google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded local tokenizer and model\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 391/391 [59:21<00:00,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "model, accuracy = main()\n",
    "print(f\"\\nFinal test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_test, y_test, label_mapping = load_data()\n",
    "\n",
    "    print(f\"Train data shape: {X_train.shape}\")\n",
    "    print(f\"Train labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "    print(f\"Number of unique linked_items: {len(label_mapping)}\")\n",
    "    if 'item_title' in y_train.columns:\n",
    "        print(f\"Number of unique item_title (train): {y_train['item_title'].nunique()}\")\n",
    "\n",
    "    # Extract labels\n",
    "    train_labels_linked = y_train['label'].astype(int).tolist()\n",
    "    test_labels_linked = y_test['label'].astype(int).tolist()\n",
    "\n",
    "    # Build item_title encoder for CoT\n",
    "    if 'item_title' in y_train.columns:\n",
    "        item_le = LabelEncoder()\n",
    "        y_train_item_ids = item_le.fit_transform(y_train['item_title'].astype(str).fillna(''))\n",
    "        # safe transform for test\n",
    "        classes_set = set(item_le.classes_)\n",
    "        y_test_item_ids = [int(item_le.transform([v])[0]) if str(v) in classes_set else 0 for v in y_test['item_title'].astype(str)]\n",
    "        num_item_classes = len(item_le.classes_)\n",
    "    else:\n",
    "        # 若不存在 item_title，则退化为单任务（item 分支只有 1 类）\n",
    "        y_train_item_ids = [0] * len(y_train)\n",
    "        y_test_item_ids = [0] * len(y_test)\n",
    "        num_item_classes = 1\n",
    "\n",
    "    # Compose input texts: case_title + performed_work\n",
    "    train_texts = (X_train.iloc[:, 0].astype(str) + ' [SEP] ' + X_train.iloc[:, 1].astype(str)).tolist()\n",
    "    test_texts = (X_test.iloc[:, 0].astype(str) + ' [SEP] ' + X_test.iloc[:, 1].astype(str)).tolist()\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load local tokenizer and build multitask model\n",
    "    local_model_path = './models/google-bert/bert-base-chinese'\n",
    "    if not os.path.isdir(local_model_path):\n",
    "        raise FileNotFoundError(f\"未找到本地模型目录: {local_model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True)\n",
    "    num_linked_classes = len(label_mapping)\n",
    "    model = MultiTaskBert(local_model_path, num_labels_linked=num_linked_classes, num_labels_item=num_item_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Datasets & loaders\n",
    "    batch_size = 16\n",
    "    max_length = 128\n",
    "    train_dataset = CoTDataset(X_train, train_labels_linked, y_train_item_ids, tokenizer, max_length)\n",
    "    test_dataset = CoTDataset(X_test, test_labels_linked, y_test_item_ids, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # Train\n",
    "    print(\"\\nTraining model (CoT)...\")\n",
    "    num_epochs = 5\n",
    "    train_losses, val_losses, val_accuracies = train_model_cot(\n",
    "        model, train_loader, test_loader, optimizer, num_epochs=num_epochs, device=device\n",
    "    )\n",
    "\n",
    "    # Plot curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training/Validation Loss'); plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Val Acc (linked)')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Validation Accuracy (linked)'); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig('bert_training_history_cot.png'); plt.show()\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    acc_linked, acc_item, preds_linked, labels_linked = evaluate_model_cot(model, test_loader, device=device)\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    save_path = 'bert_chinese_classifier_local_cot'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # 保存 backbone + heads\n",
    "    try:\n",
    "        # 仅保存 backbone 权重\n",
    "        model.bert.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\nModel backbone and tokenizer saved to '{save_path}'\")\n",
    "\n",
    "    # Confusion matrix for linked_items (top 20)\n",
    "    from collections import Counter\n",
    "    cm = confusion_matrix(labels_linked, preds_linked)\n",
    "    top_classes = np.argsort(np.bincount(labels_linked))[-20:]\n",
    "    cm_top = cm[np.ix_(top_classes, top_classes)]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_top, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Top 20 Classes, linked_items)')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.savefig('bert_confusion_matrix_cot.png'); plt.show()\n",
    "\n",
    "    print(f\"Test Accuracy (linked_items): {acc_linked:.4f}\")\n",
    "    print(f\"Test Accuracy (item_title):  {acc_item:.4f}\")\n",
    "\n",
    "    return model, acc_linked\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
