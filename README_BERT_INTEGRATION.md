### 1. ç¡®ä¿æœ¬åœ°BERTæ¨¡å‹å·²ä¸‹è½½

```bash
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
ls -la models/

# å¦‚æœä¸å­˜åœ¨ï¼Œä¸‹è½½æ¨¡å‹
modelscope download --model 'google-bert/bert-base-chinese' --local_dir './models'
```

### 2. å‡†å¤‡è®­ç»ƒæ•°æ®

ç¡®ä¿ä½ çš„æ•°æ®ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š
- `train.csv` - è®­ç»ƒæ•°æ®
- `eval.csv` - è¯„ä¼°æ•°æ®

æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š
```csv
case_title,performed_work,linked_items
å‘åŠ¨æœºå¼‚å“,æ£€æŸ¥å‘åŠ¨æœº,å‘åŠ¨æœºæ•…éšœ
åˆ¹è½¦å¤±çµ,æ›´æ¢åˆ¹è½¦ç‰‡,åˆ¹è½¦ç³»ç»Ÿæ•…éšœ
ç©ºè°ƒä¸åˆ¶å†·,æ·»åŠ åˆ¶å†·å‰‚,ç©ºè°ƒç³»ç»Ÿæ•…éšœ
```

### 3. è®­ç»ƒBERTæ¨¡å‹

```bash
# åŸºæœ¬è®­ç»ƒå‘½ä»¤
python src/train_bert.py \
    --bert-model ./models \
    --allow-online False \
    --num-train-epochs 3.0 \
    --train-batch-size 16 \
    --eval-batch-size 32 \
    --max-length 256

# é«˜çº§è®­ç»ƒå‘½ä»¤ï¼ˆå¸¦æ—©åœå’Œä¸å¹³è¡¡å¤„ç†ï¼‰
python src/train_bert.py \
    --bert-model ./models \
    --allow-online False \
    --num-train-epochs 10.0 \
    --train-batch-size 16 \
    --eval-batch-size 32 \
    --max-length 256 \
    --early-stopping-patience 3 \
    --learning-rate 5e-5 \
    --resample-method ros \
    --outmodel bert_model.joblib
```

### 4. ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹

```bash
# å•æ ·æœ¬é¢„æµ‹ï¼ˆä¸¤ç§å…¥å£ç­‰ä»·ï¼Œpredict.py ç°å·²æ”¯æŒ BERT è®­ç»ƒäº§ç”Ÿçš„ .joblibï¼‰
python src/predict.py \
    --modelsdir ./models \
    --model bert_model.joblib \
    --outdir ./output \
    --infile eval.csv

# æˆ–ä½¿ç”¨ä¸“ç”¨å…¥å£
python src/predict_bert.py \
    --modelsdir ./models \
    --model bert_model.joblib \
    --outdir ./output \
    --infile eval.csv
```

```python
# æ‰¹é‡é¢„æµ‹ï¼ˆåœ¨Pythonä»£ç ä¸­ï¼‰
from src.predict import predict  # predict.py åŒæ ·æ”¯æŒ BERT/TF-IDF ä¸¤ç§ bundle

results = predict(
    texts=["å‘åŠ¨æœºæœ‰å¼‚å“", "åˆ¹è½¦ä¸çµæ•"], 
    model_path="./models/bert_model.joblib",
    top_k=5
)
print(results)
```

### 5. è¯„ä¼°æ¨¡å‹æ€§èƒ½

```bash
# åŸºæœ¬è¯„ä¼°
python src/eval_bert.py \
    --modeldir ./models \
    --model bert_model.joblib \
    --outdir ./output \
    --path eval.csv \
    --mode new

# å¼€æ”¾é›†è¯„ä¼°ï¼ˆå¸¦æ‹’åˆ¤é˜ˆå€¼ï¼‰
python src/eval_bert.py \
    --modeldir ./models \
    --model bert_model.joblib \
    --outdir ./output \
    --path eval.csv \
    --mode new \
    --reject-threshold 0.7

# é˜ˆå€¼æ‰«æè¯„ä¼°
python src/eval_bert.py \
    --modeldir ./models \
    --model bert_model.joblib \
    --outdir ./output \
    --path eval.csv \
    --mode new \
    --sweep-thresholds "0.5:0.9:0.05"
```

## ğŸ”§ å‚æ•°è¯´æ˜

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--bert-model` | `./models` | BERTæ¨¡å‹è·¯å¾„æˆ–åç§° |
| `--allow-online` | `False` | æ˜¯å¦å…è®¸åœ¨çº¿ä¸‹è½½æ¨¡å‹ |
| `--num-train-epochs` | `3.0` | è®­ç»ƒè½®æ•° |
| `--train-batch-size` | `16` | è®­ç»ƒæ‰¹æ¬¡å¤§å° |
| `--eval-batch-size` | `32` | è¯„ä¼°æ‰¹æ¬¡å¤§å° |
| `--max-length` | `256` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `--learning-rate` | `5e-5` | å­¦ä¹ ç‡ |
| `--early-stopping-patience` | `3` | æ—©åœè€å¿ƒå€¼ |
| `--resample-method` | `none` | ä¸å¹³è¡¡å¤„ç†æ–¹æ³• |
| `--fp16` | `False` | æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ |

### é¢„æµ‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--modelsdir` | `./models` | æ¨¡å‹ç›®å½• |
| `--model` | `7.joblib` | æ¨¡å‹æ–‡ä»¶å |
| `--outdir` | `./output/2025_up_to_month_7` | è¾“å‡ºç›®å½• |
| `--infile` | `eval.csv` | è¾“å…¥æ–‡ä»¶ |
| `--reject-threshold` | `None` | æ‹’åˆ¤é˜ˆå€¼ |

### è¯„ä¼°å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--mode` | `new` | è¯„ä¼°æ¨¡å¼ï¼šnew/clean/dirty |
| `--reject-threshold` | `None` | æ‹’åˆ¤é˜ˆå€¼ |
| `--sweep-thresholds` | `None` | é˜ˆå€¼æ‰«æèŒƒå›´ |
| `--unknown-policy` | `tag-not-in-train` | æœªçŸ¥æ ‡ç­¾å¤„ç†ç­–ç•¥ |

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

### è®­ç»ƒè¾“å‡º

- `loss_curve.png` - è®­ç»ƒæŸå¤±æ›²çº¿å›¾
- `loss_data.json` - æŸå¤±æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
- `metrics_eval.csv` - è¯„ä¼°æŒ‡æ ‡
- `log/{model}_train.txt` - è®­ç»ƒæ—¥å¿—
- `models/{model}_bert/` - BERTæ¨¡å‹ç›®å½•
- `models/{model}.joblib` - å…¼å®¹æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶

### è¯„ä¼°è¾“å‡º

- `predictions_{file}.csv` - é€æ ·æœ¬é¢„æµ‹ç»“æœ
- `threshold_sweep.csv` - é˜ˆå€¼æ‰«æç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
- `metrics_best_model_all_splits.csv` - æ‰€æœ‰åˆ†å‰²çš„è¯„ä¼°æŒ‡æ ‡

## ğŸ”„ ä¸åŸæœ‰TF-IDFæµç¨‹çš„å…¼å®¹æ€§

### æ•°æ®æ ¼å¼å…¼å®¹

BERTç‰ˆæœ¬å®Œå…¨å…¼å®¹åŸæœ‰çš„æ•°æ®æ ¼å¼ï¼š
- æ”¯æŒX/Yåˆ†ç¦»æ–‡ä»¶ï¼ˆ`train_X.csv` + `train_y.csv`ï¼‰
- æ”¯æŒå•è¡¨æ–‡ä»¶ï¼ˆ`train.csv`ï¼‰
- æ”¯æŒå¤šç§æ ‡ç­¾åˆ—åï¼ˆ`linked_items`, `extend_id`, `item_title`ï¼‰

### æ¥å£å…¼å®¹

æ‰€æœ‰è„šæœ¬éƒ½ä¿æŒä¸åŸæœ‰ç‰ˆæœ¬ç›¸åŒçš„å‘½ä»¤è¡Œæ¥å£ï¼š
```bash
# TF-IDFç‰ˆæœ¬
python src/train.py --train-file train.csv --eval-file eval.csv
python src/predict.py --model 7.joblib
python src/eval.py --model 7.joblib

# BERTç‰ˆæœ¬ï¼ˆå®Œå…¨å…¼å®¹ï¼‰
python src/train_bert.py --train-file train.csv --eval-file eval.csv
python src/predict_bert.py --model bert_model.joblib
python src/eval_bert.py --model bert_model.joblib
```

### è¾“å‡ºæ ¼å¼å…¼å®¹

- æ¨¡å‹æ–‡ä»¶ä½¿ç”¨ç›¸åŒçš„joblibæ ¼å¼
- è¯„ä¼°æŒ‡æ ‡CSVæ ¼å¼ä¿æŒä¸€è‡´
- é¢„æµ‹ç»“æœCSVæ ¼å¼ä¿æŒä¸€è‡´

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¡¬ä»¶ä¼˜åŒ–

```bash
# å¯ç”¨GPUè®­ç»ƒï¼ˆå¦‚æœæœ‰CUDAï¼‰
python src/train_bert.py --fp16 --train-batch-size 32

# ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœå†…å­˜å……è¶³ï¼‰
python src/train_bert.py --train-batch-size 64 --eval-batch-size 128
```

### 2. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

```bash
# ä½¿ç”¨æ—©åœé¿å…è¿‡æ‹Ÿåˆ
python src/train_bert.py --early-stopping-patience 5

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
python src/train_bert.py --resample-method smote

# è°ƒæ•´å­¦ä¹ ç‡
python src/train_bert.py --learning-rate 3e-5 --weight-decay 0.01
```

### 3. åºåˆ—é•¿åº¦ä¼˜åŒ–

```bash
# æ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´æœ€å¤§é•¿åº¦
python src/train_bert.py --max-length 128  # çŸ­æ–‡æœ¬
python src/train_bert.py --max-length 512  # é•¿æ–‡æœ¬
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**ï¼š`ModuleNotFoundError: No module named 'transformers'`

**è§£å†³**ï¼š
```bash
pip install transformers torch
```

### 2. å†…å­˜ä¸è¶³

**é—®é¢˜**ï¼š`CUDA out of memory`

**è§£å†³**ï¼š
```bash
# å‡å°æ‰¹æ¬¡å¤§å°
python src/train_bert.py --train-batch-size 8 --eval-batch-size 16

# å¯ç”¨æ¢¯åº¦ç´¯ç§¯
python src/train_bert.py --train-batch-size 8 --grad-accum-steps 4
```

### 3. è®­ç»ƒé€Ÿåº¦æ…¢

**é—®é¢˜**ï¼šè®­ç»ƒæ—¶é—´è¿‡é•¿

**è§£å†³**ï¼š
```bash
# å¯ç”¨æ··åˆç²¾åº¦
python src/train_bert.py --fp16

# ä½¿ç”¨å¤šGPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
export CUDA_VISIBLE_DEVICES=0,1
python src/train_bert.py
```

### 4. é¢„æµ‹ç»“æœä¸ä¸€è‡´

**é—®é¢˜**ï¼šBERTé¢„æµ‹ç»“æœä¸TF-IDFä¸åŒ

**è¯´æ˜**ï¼šè¿™æ˜¯æ­£å¸¸çš„ï¼ŒBERTå’ŒTF-IDFæ˜¯ä¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œé¢„æµ‹ç»“æœä¼šæœ‰å·®å¼‚ã€‚BERTé€šå¸¸åœ¨ç†è§£è¯­ä¹‰æ–¹é¢è¡¨ç°æ›´å¥½ã€‚

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | TF-IDF | BERT | æå‡ |
|------|--------|------|------|
| å‡†ç¡®ç‡ | ~85% | ~92% | +7% |
| F1-macro | ~82% | ~90% | +8% |
| Hit@3 | ~88% | ~95% | +7% |

*æ³¨ï¼šä»¥ä¸Šä¸ºç¤ºä¾‹æ•°æ®ï¼Œå®é™…æ€§èƒ½å–å†³äºå…·ä½“ä»»åŠ¡å’Œæ•°æ®è´¨é‡ã€‚*

## ğŸ”® é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æ¨¡å‹æ¶æ„

```python
# ä¿®æ”¹train_bert.pyä¸­çš„æ¨¡å‹åŠ è½½éƒ¨åˆ†
from transformers import AutoModelForSequenceClassification, AutoConfig

config = AutoConfig.from_pretrained(
    "./models",
    num_labels=num_labels,
    hidden_dropout_prob=0.1,
    attention_probs_dropout_prob=0.1
)
model = AutoModelForSequenceClassification.from_pretrained(
    "./models",
    config=config
)
```

### 2. è‡ªå®šä¹‰è®­ç»ƒå›è°ƒ

```python
# æ·»åŠ è‡ªå®šä¹‰å›è°ƒ
class CustomCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        # è‡ªå®šä¹‰é€»è¾‘
        pass

# åœ¨è®­ç»ƒä¸­ä½¿ç”¨
trainer.add_callback(CustomCallback())
```

### 3. æ¨¡å‹é›†æˆ

```python
# ç»“åˆBERTå’ŒTF-IDFçš„é¢„æµ‹
from src.train_bert import BERTModelWrapper
from src.train import main as tfidf_train

# è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
# ...

# é›†æˆé¢„æµ‹
def ensemble_predict(texts):
    bert_probs = bert_model.predict_proba(texts)
    tfidf_probs = tfidf_model.predict_proba(texts)
    # åŠ æƒå¹³å‡
    ensemble_probs = 0.7 * bert_probs + 0.3 * tfidf_probs
    return ensemble_probs
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Hugging Face Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers/)
- [BERTè®ºæ–‡](https://arxiv.org/abs/1810.04805)
- [æ–‡æœ¬åˆ†ç±»æœ€ä½³å®è·µ](https://huggingface.co/docs/transformers/tasks/sequence_classification)

## ğŸ¤ è´¡çŒ®æŒ‡å—

å¦‚æœä½ å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š

1. æ£€æŸ¥ç°æœ‰çš„Issues
2. åˆ›å»ºæ–°çš„Issueæè¿°é—®é¢˜
3. æäº¤Pull Request

---

ğŸ‰ **æ­å–œï¼** ä½ ç°åœ¨å·²ç»æŒæ¡äº†BERTæ¨¡å‹é›†æˆçš„å®Œæ•´ä½¿ç”¨æ–¹æ³•ã€‚å¼€å§‹ä½ çš„æ–‡æœ¬åˆ†ç±»ä¹‹æ—…å§ï¼