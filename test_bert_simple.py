#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç®€åŒ–çš„BERTæµ‹è¯•è„šæœ¬ï¼Œåªæµ‹è¯•æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬é¢„æµ‹åŠŸèƒ½
"""

import os
import sys
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_bert_loading():
    """æµ‹è¯•BERTæ¨¡å‹åŠ è½½"""
    print("=== æµ‹è¯•BERTæ¨¡å‹åŠ è½½ ===")
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists('./models/google-bert/bert-base-chinese'):
        print("âœ— æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
        return False
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained('./models/google-bert/bert-base-chinese', local_files_only=True)
        print("âœ“ æˆåŠŸåŠ è½½æœ¬åœ°åˆ†è¯å™¨")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForSequenceClassification.from_pretrained(
            './models/google-bert/bert-base-chinese',
            num_labels=5,  # å‡è®¾5åˆ†ç±»ä»»åŠ¡
            ignore_mismatched_sizes=True,
            local_files_only=True
        )
        print("âœ“ æˆåŠŸåŠ è½½æœ¬åœ°BERTæ¨¡å‹")
        
        # æµ‹è¯•åŸºæœ¬åˆ†è¯åŠŸèƒ½
        test_text = "å‘åŠ¨æœºå¼‚å“ï¼Œéœ€è¦æ£€æŸ¥"
        inputs = tokenizer(test_text, return_tensors="pt")
        print(f"âœ“ åˆ†è¯æµ‹è¯•æˆåŠŸï¼Œè¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        
        # æµ‹è¯•åŸºæœ¬é¢„æµ‹åŠŸèƒ½
        import torch
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            print(f"âœ“ é¢„æµ‹æµ‹è¯•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {probabilities.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def test_bert_wrapper():
    """æµ‹è¯•BERTæ¨¡å‹åŒ…è£…å™¨"""
    print("\n=== æµ‹è¯•BERTæ¨¡å‹åŒ…è£…å™¨ ===")
    
    try:
        # å¯¼å…¥æˆ‘ä»¬çš„åŒ…è£…å™¨
        from train_bert import BERTModelWrapper
        from sklearn.preprocessing import LabelEncoder
        
        print("âœ“ æˆåŠŸå¯¼å…¥BERTModelWrapper")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ ‡ç­¾ç¼–ç å™¨
        labels = ['å‘åŠ¨æœºæ•…éšœ', 'åˆ¹è½¦ç³»ç»Ÿæ•…éšœ', 'ç©ºè°ƒç³»ç»Ÿæ•…éšœ', 'è½®èƒæ•…éšœ', 'ç”µæ°”ç³»ç»Ÿæ•…éšœ']
        le = LabelEncoder()
        le.fit(labels)
        
        # åˆ›å»ºåŒ…è£…å™¨
        wrapper = BERTModelWrapper('./models', None, le, 'cpu')
        print("âœ“ æˆåŠŸåˆ›å»ºBERTModelWrapper")
        
        # æµ‹è¯•é¢„æµ‹
        test_texts = ["å‘åŠ¨æœºæœ‰å¼‚å“", "åˆ¹è½¦ä¸çµæ•", "ç©ºè°ƒä¸åˆ¶å†·"]
        try:
            # è¿™ä¼šå¤±è´¥ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½†å¯ä»¥æµ‹è¯•æ¥å£
            probs = wrapper.predict_proba(test_texts)
            print(f"âœ“ é¢„æµ‹æµ‹è¯•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {probs.shape}")
            return True
        except Exception as e:
            print(f"âš  é¢„æµ‹æµ‹è¯•å¤±è´¥ï¼ˆé¢„æœŸï¼Œå› ä¸ºæ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰: {e}")
            # è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
            return True
        
    except ImportError as e:
        print(f"âœ— æ— æ³•å¯¼å…¥BERTModelWrapper: {e}")
        return False
    except Exception as e:
        print(f"âœ— BERTModelWrapperæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_compatibility():
    """æµ‹è¯•å…¼å®¹æ€§"""
    print("\n=== æµ‹è¯•å…¼å®¹æ€§ ===")
    
    # æµ‹è¯•æ•°æ®è¯»å–å‡½æ•°
    try:
        from train_bert import _read_split_or_combined, _choose_label_column
        print("âœ“ æˆåŠŸå¯¼å…¥æ•°æ®è¯»å–å‡½æ•°")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'case_title': ['å‘åŠ¨æœºå¼‚å“', 'åˆ¹è½¦å¤±çµ'],
            'performed_work': ['æ£€æŸ¥å‘åŠ¨æœº', 'æ›´æ¢åˆ¹è½¦ç‰‡'],
            'linked_items': ['å‘åŠ¨æœºæ•…éšœ', 'åˆ¹è½¦ç³»ç»Ÿæ•…éšœ']
        })
        
        # æµ‹è¯•æ ‡ç­¾åˆ—é€‰æ‹©
        label_col = _choose_label_column(test_data)
        print(f"âœ“ æ ‡ç­¾åˆ—é€‰æ‹©æˆåŠŸ: {label_col}")
        
        return True
        
    except ImportError as e:
        print(f"âœ— æ— æ³•å¯¼å…¥å…¼å®¹æ€§å‡½æ•°: {e}")
        return False
    except Exception as e:
        print(f"âœ— å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== BERTç®€åŒ–æµ‹è¯•å¼€å§‹ ===")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    loading_success = test_bert_loading()
    
    # æµ‹è¯•åŒ…è£…å™¨
    wrapper_success = test_bert_wrapper()
    
    # æµ‹è¯•å…¼å®¹æ€§
    compatibility_success = test_compatibility()
    
    # æ€»ç»“ç»“æœ
    print("\n=== æµ‹è¯•ç»“æœæ€»ç»“ ===")
    print(f"æ¨¡å‹åŠ è½½: {'âœ“ æˆåŠŸ' if loading_success else 'âœ— å¤±è´¥'}")
    print(f"æ¨¡å‹åŒ…è£…å™¨: {'âœ“ æˆåŠŸ' if wrapper_success else 'âœ— å¤±è´¥'}")
    print(f"å…¼å®¹æ€§: {'âœ“ æˆåŠŸ' if compatibility_success else 'âœ— å¤±è´¥'}")
    
    overall_success = loading_success and wrapper_success and compatibility_success
    print(f"æ•´ä½“: {'âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡' if overall_success else 'âœ— éƒ¨åˆ†æµ‹è¯•å¤±è´¥'}")
    
    if overall_success:
        print("\nğŸ‰ BERTé›†æˆåŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼")
        print("ä½ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®Œæ•´çš„è®­ç»ƒå’Œé¢„æµ‹:")
        print("1. è®­ç»ƒ: python src/train_bert.py --bert-model ./models/google-bert --allow-online False")
        print("2. é¢„æµ‹: python src/predict_bert.py --model your_model.joblib")
        print("3. è¯„ä¼°: python src/eval_bert.py --model your_model.joblib")
    
    return overall_success

if __name__ == "__main__":
    main()
    