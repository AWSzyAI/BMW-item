#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€çš„é¢„æµ‹è„šæœ¬ï¼Œæ”¯æŒBERTå’ŒTF-IDFæ¨¡å‹
ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨å’Œé…ç½®ç®¡ç†ç³»ç»Ÿ
"""

import os
import json
import argparse
import warnings
import time
import numpy as np
import pandas as pd
from tqdm import tqdm
from contextlib import nullcontext

# å¯¼å…¥æˆ‘ä»¬çš„ç®¡ç†ç³»ç»Ÿ
from model_manager import ModelManager
from config_manager import get_config_manager
from error_handler import get_error_handler, log_info, log_warning, log_error, handle_exception, handle_oom, retry, log_metrics, log_experiment_summary
from utils import (
    ensure_single_label, build_text, hit_at_k, fmt_sec, _flex_read_csv,
    calculate_performance_metrics, coverage_at_k, mean_reciprocal_rank, ndcg_at_k
)

warnings.filterwarnings("ignore")

# TF-IDFç›¸å…³å¯¼å…¥
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import SGDClassifier
    from sklearn.calibration import CalibratedClassifierCV
    _HAS_SKLEARN = True
except Exception:
    _HAS_SKLEARN = False

# BERTç›¸å…³å¯¼å…¥
try:
    from transformers import (
        AutoTokenizer,
        AutoModelForSequenceClassification,
        Trainer,
        TrainingArguments,
        DataCollatorWithPadding,
    )
    import torch
    _HAS_TRANSFORMERS = True
except Exception:
    _HAS_TRANSFORMERS = False


class UnifiedDataset(torch.utils.data.Dataset):
    """ç»Ÿä¸€æ•°æ®é›†ç±»ï¼Œæ”¯æŒBERTå’ŒTF-IDF"""
    def __init__(self, encodings=None, texts=None, labels=None):
        self.encodings = encodings  # BERTç¼–ç 
        self.texts = texts  # TF-IDFæ–‡æœ¬
        self.labels = labels

    def __getitem__(self, idx: int):
        if self.encodings is not None:
            # BERTæ•°æ®
            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
            if self.labels is not None:
                item["labels"] = torch.tensor(int(self.labels[idx]))
            return item
        else:
            # TF-IDFæ•°æ®
            item = {"text": self.texts[idx]}
            if self.labels is not None:
                item["label"] = self.labels[idx]
            return item

    def __len__(self) -> int:
        if self.encodings is not None:
            return len(self.encodings["input_ids"])
        else:
            return len(self.texts)


def _str2bool(v) -> bool:
    return str(v).lower() in {"1", "true", "t", "y", "yes"}


def _get_bundle_label_classes(bundle: dict) -> list[str]:
    if bundle.get("labels"):
        return list(bundle["labels"])
    le = bundle.get("label_encoder")
    if le is not None:
        return le.classes_.tolist()
    raise KeyError("æ¨¡å‹ bundle ç¼ºå°‘ labels/label_encoder")


@handle_exception
@retry(max_retries=3, delay=2.0)
def main(args):
    """ä¸»é¢„æµ‹å‡½æ•°"""
    global_start = time.time()
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨å’Œé”™è¯¯å¤„ç†å™¨
    config_manager = get_config_manager()
    config_manager.update_from_args(vars(args))
    
    error_handler = get_error_handler(
        log_file=f"./logs/predict_{time.strftime('%Y%m%d_%H%M%S')}.log",
        log_level=config_manager.get_system_config().log_level
    )
    
    # éªŒè¯é…ç½®
    if not config_manager.validate_configs():
        raise RuntimeError("é…ç½®éªŒè¯å¤±è´¥")
    
    # è·å–é…ç½®
    bert_config = config_manager.get_bert_config()
    data_config = config_manager.get_data_config()
    
    # ç¡®å®šæ¨¡å‹ç±»å‹ - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    model_type = getattr(args, 'model_type', 'bert')
    
    log_info(f"=== {model_type.upper()}æ¨¡å‹é¢„æµ‹å¼€å§‹ ===")
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    model_manager = ModelManager()
    
    # è¯»å–æ•°æ®
    log_info("ğŸ“– æ­£åœ¨è¯»å–æµ‹è¯•æ•°æ®...")
    log_info(f"   æ•°æ®ç›®å½•: {data_config.outdir}")
    # ç»Ÿä¸€ä½¿ç”¨ eval_file ä½œä¸ºé¢„æµ‹è¾“å…¥ï¼ˆä¸ eval é…ç½®ä¿æŒä¸€è‡´ï¼‰
    test_file = getattr(data_config, "test_file", None) or data_config.eval_file
    log_info(f"   æµ‹è¯•æ–‡ä»¶: {test_file}")
    df_te = _flex_read_csv(data_config.outdir, test_file)
    log_info(f"âœ“ æµ‹è¯•æ•°æ®è¯»å–å®Œæˆ: {df_te.shape}")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    for col in ["case_title", "performed_work"]:
        if col not in df_te.columns:
            raise KeyError(f"æµ‹è¯•æ•°æ®ç¼ºå°‘åˆ—ï¼š{col}")
    
    # æ„å»ºé¢„æµ‹æ–‡æœ¬
    X_te = build_text(df_te).tolist()
    
    # åŠ è½½æ¨¡å‹
    log_info(f"ğŸ”§ æ­£åœ¨åŠ è½½{model_type.upper()}æ¨¡å‹...")
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å‹æ–‡ä»¶åï¼Œä¼˜å…ˆçº§æœ€é«˜
    model_name = getattr(args, 'model', None)
    if model_name is None:
        # å¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æä¾›ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        model_name = getattr(data_config, 'model', data_config.outmodel)
    
    model_path = os.path.join(data_config.modelsdir, model_name)
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    if model_type == 'bert':
        # åŠ è½½BERTæ¨¡å‹
        model_bundle = model_manager.load_model_bundle(model_path)
        
        if model_bundle["model_type"] != "bert":
            raise ValueError(f"æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: bert, å®é™…: {model_bundle['model_type']}")

        label_classes = _get_bundle_label_classes(model_bundle)
        model_manager.setup_label_encoder(label_classes)
        
        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        model_manager.setup_tokenizer(model_bundle["model_dir"], local_files_only=True)
        model_manager.setup_model(model_bundle["model_dir"], len(label_classes), local_files_only=True)
        
        # ç¼–ç æ•°æ®
        def _tokenize(batch_texts: list[str]):
            return model_manager.tokenizer(
                batch_texts,
                padding=False,
                truncation=True,
                max_length=bert_config.max_length,
            )
        
        log_info("ğŸ”¤ æ­£åœ¨ç¼–ç æµ‹è¯•æ•°æ®...")
        log_info(f"   æœ€å¤§åºåˆ—é•¿åº¦: {bert_config.max_length}")
        enc_te = _tokenize(X_te)
        log_info(f"âœ“ æµ‹è¯•æ•°æ®ç¼–ç å®Œæˆ: {len(enc_te['input_ids'])} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        ds_te = UnifiedDataset(encodings=dict(enc_te))
        
        # åˆ›å»ºTrainer
        training_args = TrainingArguments(
            output_dir="./tmp_predict",
            per_device_eval_batch_size=bert_config.eval_batch_size,
            report_to=[],
            fp16=bert_config.fp16 and model_manager.device.type == 'cuda',
        )
        
        data_collator = DataCollatorWithPadding(model_manager.tokenizer)
        
        trainer = Trainer(
            model=model_manager.model,
            args=training_args,
            eval_dataset=ds_te,
            tokenizer=model_manager.tokenizer,
            data_collator=data_collator,
        )
        
        # é¢„æµ‹
        log_info("ğŸ”® å¼€å§‹é¢„æµ‹...")
        predict_start = time.time()
        
        with torch.no_grad():
            predictions = trainer.predict(ds_te)
        
        predict_time = time.time() - predict_start
        log_info(f"âœ“ é¢„æµ‹å®Œæˆï¼Œè€—æ—¶ {fmt_sec(predict_time)}")
        
        # å¤„ç†é¢„æµ‹ç»“æœ
        logits = predictions.predictions
        y_pred = np.argmax(logits, axis=1)
        
        # è®¡ç®—æ¦‚ç‡
        e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        proba = e / e.sum(axis=1, keepdims=True)
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        y_pred_labels = model_manager.label_encoder.inverse_transform(y_pred)
        
        # åˆ›å»ºç»“æœDataFrame
        result_df = pd.DataFrame({
            "case_id": df_te["case_id"] if "case_id" in df_te.columns else range(len(y_pred_labels)),
            "predicted_label": y_pred_labels,
        })
        
        # æ·»åŠ æ¦‚ç‡åˆ—
        for i, label in enumerate(model_manager.label_encoder.classes_):
            result_df[f"prob_{label}"] = proba[:, i]
        
        # æ·»åŠ top-ké¢„æµ‹
        for k in [1, 3, 5, 10]:
            if k <= len(model_manager.label_encoder.classes_):
                top_k_indices = np.argsort(proba, axis=1)[:, -k:][:, ::-1]
                top_k_labels = model_manager.label_encoder.inverse_transform(top_k_indices.flatten()).reshape(top_k_indices.shape)
                result_df[f"top_{k}_predictions"] = ["|".join(labels) for labels in top_k_labels]
        
    else:
        # åŠ è½½TF-IDFæ¨¡å‹
        if not _HAS_SKLEARN:
            raise RuntimeError("TF-IDFæ¨¡å‹é¢„æµ‹éœ€è¦å®‰è£…scikit-learnåº“")
        
        model_bundle = model_manager.load_model_bundle(model_path)
        
        if model_bundle["model_type"] != "tfidf":
            raise ValueError(f"æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: tfidf, å®é™…: {model_bundle['model_type']}")
        
        # æå–æ¨¡å‹ç»„ä»¶
        classifier = model_bundle["model"]
        vectorizer = model_bundle["vectorizer"]
        label_encoder = model_bundle["label_encoder"]
        
        # è®¾ç½®æ ‡ç­¾ç¼–ç å™¨
        model_manager.setup_label_encoder(label_encoder.classes_.tolist())
        
        # é¢„æµ‹
        log_info("ğŸ”® å¼€å§‹é¢„æµ‹...")
        predict_start = time.time()
        
        # ç‰¹å¾æå–
        X_te_vec = vectorizer.transform(X_te)
        
        # é¢„æµ‹
        y_pred = classifier.predict(X_te_vec)
        y_proba = classifier.decision_function(X_te_vec)
        
        # å¤„ç†æ¦‚ç‡
        if y_proba.ndim == 1:
            e = np.exp(y_proba - np.max(y_proba))
            y_proba = e / e.sum(axis=1, keepdims=True)
        else:
            y_proba = np.exp(y_proba - np.max(y_proba, axis=1, keepdims=True))
            y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)
        
        predict_time = time.time() - predict_start
        log_info(f"âœ“ é¢„æµ‹å®Œæˆï¼Œè€—æ—¶ {fmt_sec(predict_time)}")
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        y_pred_labels = label_encoder.inverse_transform(y_pred)
        
        # åˆ›å»ºç»“æœDataFrame
        result_df = pd.DataFrame({
            "case_id": df_te["case_id"] if "case_id" in df_te.columns else range(len(y_pred_labels)),
            "predicted_label": y_pred_labels,
        })
        
        # æ·»åŠ æ¦‚ç‡åˆ—
        for i, label in enumerate(label_encoder.classes_):
            result_df[f"prob_{label}"] = y_proba[:, i]
        
        # æ·»åŠ top-ké¢„æµ‹
        for k in [1, 3, 5, 10]:
            if k <= len(label_encoder.classes_):
                top_k_indices = np.argsort(y_proba, axis=1)[:, -k:][:, ::-1]
                top_k_labels = label_encoder.inverse_transform(top_k_indices.flatten()).reshape(top_k_indices.shape)
                result_df[f"top_{k}_predictions"] = ["|".join(labels) for labels in top_k_labels]
    
    # ä¿å­˜ç»“æœ
    log_info("ğŸ’¾ æ­£åœ¨ä¿å­˜é¢„æµ‹ç»“æœ...")
    # è¾“å‡ºæ–‡ä»¶åä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡å…¼å®¹ DataConfig ä¸­å¯èƒ½å­˜åœ¨çš„å­—æ®µï¼Œæœ€åå›é€€åˆ°é»˜è®¤å€¼
    output_file = getattr(args, "output_file", None) \
        or getattr(data_config, "output_file", None) \
        or "predictions.csv"
    output_path = os.path.join(data_config.outdir, output_file)
    result_df.to_csv(output_path, index=False)
    log_info(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # åˆ›å»ºJSONæ’åºæ–‡ä»¶ï¼ŒåŒ…å«é¢„æµ‹æ ‡ç­¾ã€åŸå§‹æ ‡ç­¾å’Œitem_title
    log_info("ğŸ“„ æ­£åœ¨ç”ŸæˆJSONæ’åºæ–‡ä»¶...")
    
    # è¯»å–æ ‡ç­¾æ˜ å°„æ–‡ä»¶
    label_mapping_path = os.path.join(data_config.outdir, "label_mapping.csv")
    if os.path.exists(label_mapping_path):
        label_mapping_df = pd.read_csv(label_mapping_path)
        # åˆ›å»ºæ ‡ç­¾åˆ°item_titleçš„æ˜ å°„
        label_to_title = dict(zip(label_mapping_df['linked_items'], label_mapping_df['item_title']))
    else:
        log_warning(f"æœªæ‰¾åˆ°æ ‡ç­¾æ˜ å°„æ–‡ä»¶: {label_mapping_path}")
        label_to_title = {}
    
    # åˆ›å»ºJSONæ ¼å¼çš„é¢„æµ‹ç»“æœ
    predictions_json = []
    for idx, row in result_df.iterrows():
        pred_label = row['predicted_label']
        item_title = label_to_title.get(pred_label, "")
        
        # è·å–top-ké¢„æµ‹çš„item_title
        top_k_titles = []
        for k in [1, 3, 5, 10]:
            if f"top_{k}_predictions" in row:
                top_labels = row[f"top_{k}_predictions"].split("|")
                top_titles = [label_to_title.get(label, "") for label in top_labels]
                top_k_titles.append({
                    f"top_{k}_predictions": top_labels,
                    f"top_{k}_titles": top_titles
                })
        
        prediction_item = {
            "case_id": row['case_id'] if 'case_id' in row else idx,
            "predicted_label": pred_label,
            "predicted_item_title": item_title,
            "confidence": float(row[f"prob_{pred_label}"]) if f"prob_{pred_label}" in row else 0.0,
        }
        
        # æ·»åŠ æ‰€æœ‰æ¦‚ç‡ä¿¡æ¯
        for col in result_df.columns:
            if col.startswith('prob_'):
                label = col.replace('prob_', '')
                prediction_item[f"prob_{label}"] = float(row[col])
        
        # æ·»åŠ top-kä¿¡æ¯
        for top_k_info in top_k_titles:
            prediction_item.update(top_k_info)
        
        predictions_json.append(prediction_item)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    predictions_json.sort(key=lambda x: x['confidence'], reverse=True)
    
    # ä¿å­˜JSONæ–‡ä»¶
    json_output_path = os.path.join(data_config.outdir, "predictions_sorted.json")
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(predictions_json, f, ensure_ascii=False, indent=2)
    
    log_info(f"âœ“ JSONæ’åºæ–‡ä»¶å·²ä¿å­˜åˆ°: {json_output_path}")
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    performance_metrics = calculate_performance_metrics(predict_start, time.time(), len(result_df))
    
    # è®¡ç®—é¢„æµ‹è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰æ¦‚ç‡ä¿¡æ¯ï¼‰
    quality_metrics = {}
    if model_type == 'bert':
        proba_matrix = np.array([result_df[f"prob_{label}"].values for label in model_manager.label_encoder.classes_]).T
    else:
        proba_matrix = np.array([result_df[f"prob_{label}"].values for label in label_encoder.classes_]).T
    
    # è®¡ç®—è¦†ç›–ç‡æŒ‡æ ‡
    quality_metrics["coverage@3"] = coverage_at_k(proba_matrix, 3)
    quality_metrics["coverage@5"] = coverage_at_k(proba_matrix, 5)
    quality_metrics["coverage@10"] = coverage_at_k(proba_matrix, 10)
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    quality_metrics["avg_confidence"] = np.mean(np.max(proba_matrix, axis=1))
    quality_metrics["min_confidence"] = np.min(np.max(proba_matrix, axis=1))
    quality_metrics["max_confidence"] = np.max(np.max(proba_matrix, axis=1))
    
    # è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
    confidence_thresholds = [0.5, 0.7, 0.8, 0.9, 0.95]
    confidence_distribution = {}
    for threshold in confidence_thresholds:
        count = np.sum(np.max(proba_matrix, axis=1) >= threshold)
        confidence_distribution[f"confidence_{threshold}+"] = {
            "count": int(count),
            "percentage": float(count / len(result_df) * 100)
        }
    
    # ä½¿ç”¨æ–°çš„æ—¥å¿—è®°å½•åŠŸèƒ½è®°å½•æ€§èƒ½æŒ‡æ ‡
    log_metrics(performance_metrics, "performance")
    
    # è®°å½•é¢„æµ‹è´¨é‡æŒ‡æ ‡
    log_metrics(quality_metrics, "quality")
    
    # è®°å½•ç½®ä¿¡åº¦åˆ†å¸ƒ
    log_metrics(confidence_distribution, "confidence")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    log_info("\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
    log_info(f"  æ€»æ ·æœ¬æ•°: {len(result_df)}")
    log_info(f"  é¢„æµ‹ç±»åˆ«æ•°: {len(result_df['predicted_label'].unique())}")
    
    # æ˜¾ç¤ºé¢„æµ‹åˆ†å¸ƒ
    label_counts = result_df['predicted_label'].value_counts()
    log_info("\nğŸ“ˆ é¢„æµ‹åˆ†å¸ƒ (å‰10ä¸ª):")
    for label, count in label_counts.head(10).items():
        item_title = label_to_title.get(label, "")
        log_info(f"  {label} ({item_title}): {count}")
    
    # ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ°æ–‡ä»¶
    performance_df = pd.DataFrame([{
        **performance_metrics,
        **quality_metrics,
        "model_type": model_type,
        "model_path": model_path,
        "total_samples": len(result_df),
        "unique_predictions": len(result_df['predicted_label'].unique()),
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
    }])
    
    performance_path = os.path.join(data_config.outdir, "performance_metrics.csv")
    performance_df.to_csv(performance_path, index=False)
    log_info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {performance_path}")
    
    # ä¿å­˜ç½®ä¿¡åº¦åˆ†å¸ƒåˆ°æ–‡ä»¶
    confidence_df = pd.DataFrame([
        {
            "threshold": threshold,
            "count": info["count"],
            "percentage": info["percentage"]
        }
        for threshold, info in confidence_distribution.items()
    ])
    
    confidence_path = os.path.join(data_config.outdir, "confidence_distribution.csv")
    confidence_df.to_csv(confidence_path, index=False)
    log_info(f"ğŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒå·²ä¿å­˜åˆ°: {confidence_path}")
    
    # è®°å½•å®éªŒæ‘˜è¦
    experiment_info = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'model_type': model_type,
        'model_path': model_path,
        'data_info': {
            'test_samples': len(result_df),
            'unique_predictions': len(result_df['predicted_label'].unique())
        },
        'key_metrics': {
            'avg_latency': performance_metrics['avg_latency'],
            'tps': performance_metrics['tps'],
            'avg_confidence': quality_metrics['avg_confidence'],
            'coverage@5': quality_metrics['coverage@5']
        },
        'output_files': {
            'csv_results': output_path,
            'json_results': json_output_path,
            'performance_metrics': performance_path,
            'confidence_distribution': confidence_path
        }
    }
    
    log_experiment_summary(experiment_info)
    
    total_time = time.time() - global_start
    log_info(f"\nğŸ‰ {model_type.upper()}é¢„æµ‹å®Œæˆï¼")
    log_info(f"â±ï¸  æ€»è€—æ—¶ï¼š{fmt_sec(total_time)}")
    log_info(f"ğŸ“ CSVç»“æœæ–‡ä»¶ï¼š{output_path}")
    log_info(f"ğŸ“„ JSONæ’åºæ–‡ä»¶ï¼š{json_output_path}")
    log_info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ–‡ä»¶ï¼š{performance_path}")
    log_info(f"ğŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒæ–‡ä»¶ï¼š{confidence_path}")
    
    # æ¸…ç†å†…å­˜
    model_manager.clear_memory()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    # æ¨¡å‹ç±»å‹é€‰æ‹© - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    parser.add_argument("--model-type", type=str, default="bert", choices=["bert", "tfidf"], help="æ¨¡å‹ç±»å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--test-file", type=str, default="test.csv", help="æµ‹è¯•é›†æ–‡ä»¶å")
    parser.add_argument("--outdir", type=str, default="./output/2025_up_to_month_2", help="æ•°æ®ç›®å½•")
    parser.add_argument("--modelsdir", type=str, default="./models", help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--model", type=str, required=True, help="æ¨¡å‹æ–‡ä»¶å")
    parser.add_argument("--output-file", type=str, default="predictions.csv", help="é¢„æµ‹ç»“æœæ–‡ä»¶å")
    
    # BERT å‚æ•°
    parser.add_argument("--bert-model", type=str, default="./models/google-bert/bert-base-chinese", help="BERTæ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--eval-batch-size", type=int, default=32)
    parser.add_argument("--max-length", type=int, default=256)
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦é¢„æµ‹")
    parser.add_argument("--allow-online", type=_str2bool, default=False, help="å…è®¸åœ¨çº¿ä¸‹è½½HFæ¨¡å‹")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--log-level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    
    args = parser.parse_args()
    main(args)