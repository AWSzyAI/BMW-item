#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€çš„è¯„ä¼°è„šæœ¬ï¼Œæ”¯æŒBERTå’ŒTF-IDFæ¨¡å‹
ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨å’Œé…ç½®ç®¡ç†ç³»ç»Ÿ
"""

import os
import json
import argparse
import warnings
import time
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import log_loss, accuracy_score, f1_score, classification_report, confusion_matrix
from tqdm import tqdm
from contextlib import nullcontext

# å¯¼å…¥æˆ‘ä»¬çš„ç®¡ç†ç³»ç»Ÿ
from model_manager import ModelManager
from config_manager import get_config_manager
from error_handler import get_error_handler, log_info, log_warning, log_error, handle_exception, handle_oom, retry, log_metrics, log_experiment_summary
from utils import (
    ensure_single_label, build_text, hit_at_k, fmt_sec, _flex_read_csv,
    mean_reciprocal_rank, ndcg_at_k, coverage_at_k, calculate_error_analysis,
    calculate_top_n_distribution, calculate_performance_metrics
)

warnings.filterwarnings("ignore")

# TF-IDFç›¸å…³å¯¼å…¥
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import SGDClassifier
    from sklearn.calibration import CalibratedClassifierCV
    _HAS_SKLEARN = True
except Exception:
    _HAS_SKLEARN = False

# BERTç›¸å…³å¯¼å…¥
try:
    from transformers import (
        AutoTokenizer,
        AutoModelForSequenceClassification,
        Trainer,
        TrainingArguments,
        DataCollatorWithPadding,
    )
    import torch
    _HAS_TRANSFORMERS = True
except Exception:
    _HAS_TRANSFORMERS = False


class UnifiedDataset(torch.utils.data.Dataset):
    """ç»Ÿä¸€æ•°æ®é›†ç±»ï¼Œæ”¯æŒBERTå’ŒTF-IDF"""
    def __init__(self, encodings=None, texts=None, labels=None):
        self.encodings = encodings  # BERTç¼–ç 
        self.texts = texts  # TF-IDFæ–‡æœ¬
        self.labels = labels

    def __getitem__(self, idx: int):
        if self.encodings is not None:
            # BERTæ•°æ®
            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
            if self.labels is not None:
                item["labels"] = torch.tensor(int(self.labels[idx]))
            return item
        else:
            # TF-IDFæ•°æ®
            item = {"text": self.texts[idx]}
            if self.labels is not None:
                item["label"] = self.labels[idx]
            return item

    def __len__(self) -> int:
        if self.encodings is not None:
            return len(self.encodings["input_ids"])
        else:
            return len(self.texts)


def _compute_metrics(eval_pred, num_labels: int) -> dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1w = f1_score(labels, preds, average="weighted")
    f1m = f1_score(labels, preds, average="macro")
    
    # è®¡ç®—æ¦‚ç‡
    e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    proba = e / e.sum(axis=1, keepdims=True)
    
    # è®¡ç®—åŸºç¡€æŒ‡æ ‡
    out = {
        "accuracy": float(acc),
        "f1_weighted": float(f1w),
        "f1_macro": float(f1m),
        "hit@1": hit_at_k(labels, proba, 1),
        "hit@3": hit_at_k(labels, proba, 3),
        "hit@5": hit_at_k(labels, proba, 5) if num_labels >= 5 else float("nan"),
        "hit@10": hit_at_k(labels, proba, 10) if num_labels >= 10 else float("nan"),
    }
    
    # è®¡ç®—é«˜çº§æŒ‡æ ‡
    out["mrr"] = mean_reciprocal_rank(labels, proba)
    out["ndcg@3"] = ndcg_at_k(labels, proba, 3)
    out["ndcg@5"] = ndcg_at_k(labels, proba, 5)
    out["ndcg@10"] = ndcg_at_k(labels, proba, 10)
    out["coverage@3"] = coverage_at_k(proba, 3)
    out["coverage@5"] = coverage_at_k(proba, 5)
    out["coverage@10"] = coverage_at_k(proba, 10)
    
    return out


def _str2bool(v) -> bool:
    return str(v).lower() in {"1", "true", "t", "y", "yes"}


def _read_split_or_combined(base_dir: str, base_filename: str) -> pd.DataFrame:
    """ä¼˜å…ˆè¯»å– X/Y åˆ†ç¦»æ–‡ä»¶ï¼›è‹¥ä¸å­˜åœ¨åˆ™å›é€€åˆ°å•è¡¨ CSVã€‚"""
    base_dir = os.path.abspath(base_dir)
    name = os.path.basename(base_filename)
    stem, ext = os.path.splitext(name)
    # å…¼å®¹ä¼ å…¥ *_X.csv æˆ– *_y.csv çš„æƒ…å†µï¼Œç»Ÿä¸€å›åˆ°å…¬å…± stem
    if stem.endswith("_X"):
        stem = stem[:-2]
    if stem.endswith("_y"):
        stem = stem[:-2]

    x_name = f"{stem}_X.csv"
    y_name = f"{stem}_y.csv"

    def _exists_in_dir(fname: str) -> str | None:
        p = os.path.join(base_dir, fname)
        return p if os.path.exists(p) else None

    # 1) ä¼˜å…ˆå°è¯•åˆ†è¡¨
    x_path = _exists_in_dir(x_name)
    y_path = _exists_in_dir(y_name)
    if x_path and y_path:
        X = _flex_read_csv(base_dir, os.path.basename(x_path))
        y = _flex_read_csv(base_dir, os.path.basename(y_name))

        # å…¼å®¹ y åˆ—å
        if "linked_items" not in y.columns:
            if "label" in y.columns:
                y = y.rename(columns={"label": "linked_items"})
            elif "y" in y.columns:
                y = y.rename(columns={"y": "linked_items"})
            else:
                # è‹¥å¤šåˆ—ï¼Œå–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡ç­¾
                first_label_col = y.columns[0]
                warnings.warn(f"æœªæ‰¾åˆ° 'linked_items'ï¼Œä½¿ç”¨ '{first_label_col}' ä½œä¸ºæ ‡ç­¾åˆ—")
                y = y.rename(columns={first_label_col: "linked_items"})

        # åªä¿ç•™æ ‡ç­¾åˆ—
        y = y[["linked_items"]]
        if len(X) != len(y):
            raise ValueError(f"X/Y è¡Œæ•°ä¸ä¸€è‡´ï¼šX={len(X)} Y={len(y)}ï¼ˆstem={stem}ï¼‰")
        df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)
        return df

    # 2) å›é€€ï¼šè¯»å–å•è¡¨ï¼ˆä¾‹å¦‚ train.csv / eval.csvï¼‰
    warnings.warn(
        f"æœªæ‰¾åˆ°åˆ†è¡¨ {x_name}+{y_name}ï¼Œå›é€€åˆ°å•è¡¨ {name}ï¼ˆç›®å½•ï¼š{base_dir}ï¼‰"
    )
    return _flex_read_csv(base_dir, name)


def _choose_label_column(df: pd.DataFrame) -> str:
    """é€‰æ‹©æ ‡ç­¾åˆ—"""
    # ä¼˜å…ˆçº§ï¼šextend_id > linked_items > item_title
    for col in ["extend_id", "linked_items", "item_title"]:
        if col in df.columns:
            return col
    raise KeyError("æœªæ‰¾åˆ°å¯ç”¨æ ‡ç­¾åˆ—ï¼ˆextend_id/linked_items/item_titleï¼‰")


def _export_open_set_predictions(
    df_ev: pd.DataFrame,
    outdir: str,
    y_raw: list[str],
    y_proba_all: np.ndarray,
    label_encoder: LabelEncoder,
    not_in_train_label: str = "__NOT_IN_TRAIN__",
    other_label: str = "__OTHER__",
    unknown_policy: str = "tag-not-in-train",
) -> None:
    """å¤ç”¨ eval.py çš„é€æ ·æœ¬é¢„æµ‹å¯¼å‡ºé€»è¾‘ï¼Œç”Ÿæˆ predictions_eval.csv é£æ ¼æ–‡ä»¶ã€‚

    - ä½¿ç”¨ BERT çš„æ¦‚ç‡ y_proba_all å’Œ label_encoder
    - ç»“æ„å¯¹é½ eval.py çš„ "new" æ¨¡å¼å¯¼å‡ºçš„ predictions_<base>.csv
    """

    cls = label_encoder.classes_
    cls_set = set(cls)
    n = len(df_ev)
    topk = min(10, len(cls))

    # Top-k é¢„æµ‹
    topk_idx = np.argsort(-y_proba_all, axis=1)[:, :topk]
    topk_labels = [[str(cls[j]) for j in row] for row in topk_idx]
    topk_scores = [[float(y_proba_all[i, j]) for j in topk_idx[i]] for i in range(n)]
    pred_top1 = [labels[0] if labels else "" for labels in topk_labels]

    # å‘½ä¸­ç‡ï¼šè‹¥ unknown_policy=map-to-otherï¼Œåˆ™æ˜ å°„æœªçŸ¥æ ‡ç­¾ï¼›å¦åˆ™æœªçŸ¥æ ‡ç­¾å‘½ä¸­ç½® NaN
    true_labels_orig = [str(t) for t in y_raw]

    use_mapping = (unknown_policy == "map-to-other" and (other_label in cls_set))
    if use_mapping:
        true_labels_mapped = [t if t in cls_set else other_label for t in true_labels_orig]
    else:
        true_labels_mapped = true_labels_orig

    def _hit_at(preds, true, k):
        if (not use_mapping) and (true not in cls_set):
            return np.nan
        return 1 if true in preds[:k] else 0

    rows = []
    for i in range(n):
        preds_i = topk_labels[i]
        scores_i = topk_scores[i]
        true_i = true_labels_orig[i]
        true_i_m = true_labels_mapped[i]
        rows.append({
            "index": i,
            "case_id": df_ev.iloc[i].get("case_id"),
            "true_label": true_i,
            "true_label_mapped": true_i_m if true_i_m != true_i else "",
            "true_in_train": (true_i in cls_set),
            "pred_top1": pred_top1[i],
            "preds_top10": "|".join(preds_i),
            "scores_top10": "|".join(f"{s:.6f}" for s in scores_i),
            "hit@1": _hit_at(preds_i, true_i_m, 1),
            "hit@3": _hit_at(preds_i, true_i_m, 3),
            "hit@5": _hit_at(preds_i, true_i_m, 5),
            "hit@10": _hit_at(preds_i, true_i_m, 10),
        })

    pred_df = pd.DataFrame(rows)
    pred_out = os.path.join(outdir, "predictions_eval.csv")
    pred_df.to_csv(pred_out, index=False, encoding="utf-8-sig")
    log_info(f"é€æ ·æœ¬é¢„æµ‹å·²ä¿å­˜ï¼š{pred_out}")


@handle_exception
@retry(max_retries=3, delay=2.0)
def main(args):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    global_start = time.time()
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨å’Œé”™è¯¯å¤„ç†å™¨
    config_manager = get_config_manager()
    config_manager.update_from_args(vars(args))
    
    error_handler = get_error_handler(
        log_file=f"./logs/eval_{time.strftime('%Y%m%d_%H%M%S')}.log",
        log_level=config_manager.get_system_config().log_level
    )
    
    # éªŒè¯é…ç½®
    if not config_manager.validate_configs():
        raise RuntimeError("é…ç½®éªŒè¯å¤±è´¥")
    
    # è·å–é…ç½®
    bert_config = config_manager.get_bert_config()
    data_config = config_manager.get_data_config()
    
    # ç¡®å®šæ¨¡å‹ç±»å‹ - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    model_type = getattr(args, 'model_type', 'bert')
    
    log_info(f"=== {model_type.upper()}æ¨¡å‹è¯„ä¼°å¼€å§‹ ===")
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    model_manager = ModelManager()
    
    # è¯»å–æ•°æ®
    log_info("ğŸ“– æ­£åœ¨è¯»å–è¯„ä¼°æ•°æ®...")
    log_info(f"   æ•°æ®ç›®å½•: {data_config.outdir}")
    log_info(f"   è¯„ä¼°æ–‡ä»¶: {data_config.eval_file}")
    df_ev = _read_split_or_combined(data_config.outdir, data_config.eval_file)
    log_info(f"âœ“ è¯„ä¼°æ•°æ®è¯»å–å®Œæˆ: {df_ev.shape}")
    
    label_col = _choose_label_column(df_ev)
    log_info(f"âœ“ é€‰æ‹©æ ‡ç­¾åˆ—: {label_col}")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    for col in ["case_title", "performed_work", label_col]:
        if col not in df_ev.columns:
            raise KeyError(f"è¯„ä¼°æ•°æ®ç¼ºå°‘åˆ—ï¼š{col}")
    
    # æ¸…æ´—æ ‡ç­¾
    df_ev[label_col] = df_ev[label_col].apply(ensure_single_label).astype(str)
    
    X_ev = build_text(df_ev).tolist()
    y_ev_raw = df_ev[label_col].astype(str).tolist()
    
    # åŠ è½½æ¨¡å‹
    log_info(f"ğŸ”§ æ­£åœ¨åŠ è½½{model_type.upper()}æ¨¡å‹...")
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ¨¡å‹æ–‡ä»¶åï¼Œä¼˜å…ˆçº§æœ€é«˜
    model_name = getattr(args, 'model', None)
    if model_name is None:
        # å¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æä¾›ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        model_name = getattr(data_config, 'model', data_config.outmodel)
    
    model_path = os.path.join(data_config.modelsdir, model_name)
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    if model_type == 'bert':
        # åŠ è½½BERTæ¨¡å‹
        model_bundle = model_manager.load_model_bundle(model_path)
        
        if model_bundle["model_type"] != "bert":
            raise ValueError(f"æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: bert, å®é™…: {model_bundle['model_type']}")
        
        # è®¾ç½®æ ‡ç­¾ç¼–ç å™¨
        if "labels" in model_bundle:
            model_manager.setup_label_encoder(model_bundle["labels"])
        elif "label_encoder" in model_bundle:
            # å¦‚æœbundleä¸­åŒ…å«label_encoderå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨å…¶classes_
            model_manager.setup_label_encoder(model_bundle["label_encoder"].classes_.tolist())
        else:
            raise KeyError("æ¨¡å‹bundleä¸­æœªæ‰¾åˆ°labelsæˆ–label_encoderé”®")
        
        # è¿‡æ»¤ä¸åœ¨è®­ç»ƒæ ‡ç­¾é›†çš„æ ·æœ¬
        ev_mask = [lbl in set(model_bundle["labels"]) for lbl in y_ev_raw]
        if not all(ev_mask):
            dropped = int(np.sum(~np.array(ev_mask)))
            log_info(f"[è­¦å‘Š] eval ä¸­æœ‰ {dropped} æ¡æ ·æœ¬çš„æ ‡ç­¾æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼ˆå°†è¢«è¿‡æ»¤ï¼‰")
        
        X_ev_f = [t for t, m in zip(X_ev, ev_mask) if m]
        y_ev_f = [l for l, m in zip(y_ev_raw, ev_mask) if m]
        y_ev = model_manager.label_encoder.transform(y_ev_f) if len(y_ev_f) > 0 else np.array([])
        
        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        model_manager.setup_tokenizer(model_bundle["model_dir"], local_files_only=True)
        model_manager.setup_model(model_bundle["model_dir"], len(model_bundle["labels"]), local_files_only=True)
        
        # ç¼–ç æ•°æ®
        def _tokenize(batch_texts: list[str]):
            return model_manager.tokenizer(
                batch_texts,
                padding=False,
                truncation=True,
                max_length=bert_config.max_length,
            )
        
        log_info("ğŸ”¤ æ­£åœ¨ç¼–ç è¯„ä¼°æ•°æ®...")
        log_info(f"   æœ€å¤§åºåˆ—é•¿åº¦: {bert_config.max_length}")
        enc_ev = _tokenize(X_ev_f)
        log_info(f"âœ“ è¯„ä¼°æ•°æ®ç¼–ç å®Œæˆ: {len(enc_ev['input_ids'])} æ ·æœ¬")
        
        # åˆ›å»ºæ•°æ®é›†
        ds_ev = UnifiedDataset(encodings=dict(enc_ev), labels=np.asarray(y_ev))
        
        # åˆ›å»ºTrainer
        training_args = TrainingArguments(
            output_dir="./tmp_eval",
            per_device_eval_batch_size=bert_config.eval_batch_size,
            report_to=[],
            fp16=bert_config.fp16 and model_manager.device.type == 'cuda',
        )
        
        data_collator = DataCollatorWithPadding(model_manager.tokenizer)
        
        trainer = Trainer(
            model=model_manager.model,
            args=training_args,
            eval_dataset=ds_ev,
            tokenizer=model_manager.tokenizer,
            data_collator=data_collator,
            compute_metrics=(lambda p: _compute_metrics(p, len(model_bundle["labels"]))),
        )
        
        # è¯„ä¼°
        log_info("ğŸ“Š å¼€å§‹è¯„ä¼°...")
        eval_start = time.time()
        
        with torch.no_grad():
            predictions = trainer.evaluate()
        
        eval_time = time.time() - eval_start
        log_info(f"âœ“ è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {fmt_sec(eval_time)}")
        
        # è·å–é¢„æµ‹ç»“æœ
        pred_output = trainer.predict(ds_ev)
        logits = pred_output.predictions
        y_pred = np.argmax(logits, axis=1)
        
        # è®¡ç®—æ¦‚ç‡
        e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        proba = e / e.sum(axis=1, keepdims=True)
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        y_pred_labels = model_manager.label_encoder.inverse_transform(y_pred)
        y_true_labels = model_manager.label_encoder.inverse_transform(y_ev)
        
        # æå–è¯„ä¼°æŒ‡æ ‡
        eval_metrics = {}
        for k in ["accuracy", "f1_weighted", "f1_macro", "hit@1", "hit@3", "hit@5", "hit@10",
                  "mrr", "ndcg@3", "ndcg@5", "ndcg@10", "coverage@3", "coverage@5", "coverage@10"]:
            if k in predictions:
                eval_metrics[k] = float(predictions[k])
        
        if "eval_loss" in predictions:
            eval_metrics["eval_loss"] = float(predictions["eval_loss"])
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_metrics = calculate_performance_metrics(eval_start, time.time(), len(y_ev))
        eval_metrics.update(performance_metrics)

        # ç”Ÿæˆä¸ eval.py å¯¹é½çš„é€æ ·æœ¬é¢„æµ‹æ–‡ä»¶ï¼Œä¾›å¼€æ”¾é›†è¯„ä¼°å’Œ rerank ä½¿ç”¨
        _export_open_set_predictions(
            df_ev=df_ev,
            outdir=data_config.outdir,
            y_raw=y_ev_f,
            y_proba_all=proba,
            label_encoder=model_manager.label_encoder,
        )
        
    else:
        # åŠ è½½TF-IDFæ¨¡å‹
        if not _HAS_SKLEARN:
            raise RuntimeError("TF-IDFæ¨¡å‹è¯„ä¼°éœ€è¦å®‰è£…scikit-learnåº“")
        
        model_bundle = model_manager.load_model_bundle(model_path)
        
        if model_bundle["model_type"] != "tfidf":
            raise ValueError(f"æ¨¡å‹ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: tfidf, å®é™…: {model_bundle['model_type']}")
        
        # æå–æ¨¡å‹ç»„ä»¶
        classifier = model_bundle["model"]
        vectorizer = model_bundle["vectorizer"]
        label_encoder = model_bundle["label_encoder"]
        
        # è®¾ç½®æ ‡ç­¾ç¼–ç å™¨
        model_manager.setup_label_encoder(label_encoder.classes_.tolist())
        
        # è¿‡æ»¤ä¸åœ¨è®­ç»ƒæ ‡ç­¾é›†çš„æ ·æœ¬
        ev_mask = [lbl in set(label_encoder.classes_) for lbl in y_ev_raw]
        if not all(ev_mask):
            dropped = int(np.sum(~np.array(ev_mask)))
            log_info(f"[è­¦å‘Š] eval ä¸­æœ‰ {dropped} æ¡æ ·æœ¬çš„æ ‡ç­¾æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼ˆå°†è¢«è¿‡æ»¤ï¼‰")
        
        X_ev_f = [t for t, m in zip(X_ev, ev_mask) if m]
        y_ev_f = [l for l, m in zip(y_ev_raw, ev_mask) if m]
        y_ev = label_encoder.transform(y_ev_f) if len(y_ev_f) > 0 else np.array([])
        
        # è¯„ä¼°
        log_info("ğŸ“Š å¼€å§‹è¯„ä¼°TF-IDFæ¨¡å‹...")
        eval_start = time.time()
        
        # ç‰¹å¾æå–
        X_ev_vec = vectorizer.transform(X_ev_f)
        
        # é¢„æµ‹
        y_pred = classifier.predict(X_ev_vec)
        y_proba = classifier.decision_function(X_ev_vec)
        
        # å¤„ç†æ¦‚ç‡
        if y_proba.ndim == 1:
            e = np.exp(y_proba - np.max(y_proba))
            y_proba = e / e.sum(axis=1, keepdims=True)
        else:
            y_proba = np.exp(y_proba - np.max(y_proba, axis=1, keepdims=True))
            y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)
        
        eval_time = time.time() - eval_start
        log_info(f"âœ“ è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {fmt_sec(eval_time)}")
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        y_pred_labels = label_encoder.inverse_transform(y_pred)
        y_true_labels = label_encoder.inverse_transform(y_ev)
        
        # è®¡ç®—æŒ‡æ ‡
        acc = accuracy_score(y_ev, y_pred)
        f1w = f1_score(y_ev, y_pred, average="weighted")
        f1m = f1_score(y_ev, y_pred, average="macro")
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        eval_metrics = {
            "accuracy": float(acc),
            "f1_weighted": float(f1w),
            "f1_macro": float(f1m),
            "hit@1": hit_at_k(y_ev, y_proba, 1),
            "hit@3": hit_at_k(y_ev, y_proba, 3),
            "hit@5": hit_at_k(y_ev, y_proba, 5),
            "hit@10": hit_at_k(y_ev, y_proba, 10),
        }
        
        # è®¡ç®—é«˜çº§æŒ‡æ ‡
        eval_metrics["mrr"] = mean_reciprocal_rank(y_ev, y_proba)
        eval_metrics["ndcg@3"] = ndcg_at_k(y_ev, y_proba, 3)
        eval_metrics["ndcg@5"] = ndcg_at_k(y_ev, y_proba, 5)
        eval_metrics["ndcg@10"] = ndcg_at_k(y_ev, y_proba, 10)
        eval_metrics["coverage@3"] = coverage_at_k(y_proba, 3)
        eval_metrics["coverage@5"] = coverage_at_k(y_proba, 5)
        eval_metrics["coverage@10"] = coverage_at_k(y_proba, 10)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_metrics = calculate_performance_metrics(eval_start, time.time(), len(y_ev))
        eval_metrics.update(performance_metrics)

        # ç”Ÿæˆä¸ eval.py å¯¹é½çš„é€æ ·æœ¬é¢„æµ‹æ–‡ä»¶ï¼Œä¾›å¼€æ”¾é›†è¯„ä¼°å’Œ rerank ä½¿ç”¨
        _export_open_set_predictions(
            df_ev=df_ev,
            outdir=data_config.outdir,
            y_raw=y_ev_f,
            y_proba_all=y_proba,
            label_encoder=label_encoder,
        )
    
    # ä½¿ç”¨æ–°çš„æ—¥å¿—è®°å½•åŠŸèƒ½è®°å½•è¯„ä¼°æŒ‡æ ‡
    log_metrics(eval_metrics, "quality")
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    performance_metrics = {k: v for k, v in eval_metrics.items()
                         if k in ['avg_latency', 'tp99_latency', 'tps', 'total_time', 'num_samples']}
    if performance_metrics:
        log_metrics(performance_metrics, "performance")
    
    # é¢„å…ˆå‡†å¤‡ label_classesï¼Œä¾›è¯¦ç»†æŠ¥å‘Šå’Œå®éªŒæ‘˜è¦å…±ç”¨
    if model_type == 'bert':
        label_classes = model_manager.label_encoder.classes_
    else:
        label_classes = label_encoder.classes_

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
    if args.detailed_report:
        log_info("\nğŸ“‹ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_true_labels, y_pred_labels, output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true_labels, y_pred_labels)
        cm_df = pd.DataFrame(cm, index=label_classes, columns=label_classes)
        
        # é”™è¯¯ç±»å‹åˆ†æ
        log_info("ğŸ” åˆ†æé”™è¯¯ç±»å‹...")
        error_analysis = calculate_error_analysis(
            np.array(y_true_labels),
            np.array(y_pred_labels),
            X_ev_f
        )
        
        # Top-Nåˆ†å¸ƒåˆ†æ
        log_info("ğŸ“Š åˆ†æTop-Nåˆ†å¸ƒ...")
        top_n_distribution = calculate_top_n_distribution(
            proba if model_type == 'bert' else y_proba,
            np.array(y_ev),
            label_classes.tolist()
        )
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        output_dir = data_config.experiment_outdir or data_config.outdir
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_path = os.path.join(output_dir, "eval_metrics.csv")
        pd.DataFrame([eval_metrics]).to_csv(metrics_path, index=False)
        log_info(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
        
        # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
        report_path = os.path.join(output_dir, "classification_report.csv")
        report_df.to_csv(report_path)
        log_info(f"ğŸ“‹ åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # ä¿å­˜æ··æ·†çŸ©é˜µ
        cm_path = os.path.join(output_dir, "confusion_matrix.csv")
        cm_df.to_csv(cm_path)
        log_info(f"ğŸ”¢ æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_path}")
        
        # ä¿å­˜é”™è¯¯åˆ†æ
        error_analysis_df = pd.DataFrame([
            {
                'error_type': error_type,
                'count': info['count'],
                'percentage': info['percentage']
            }
            for error_type, info in error_analysis.items()
        ])
        error_analysis_path = os.path.join(output_dir, "error_analysis.csv")
        error_analysis_df.to_csv(error_analysis_path, index=False)
        log_info(f"âŒ é”™è¯¯åˆ†æå·²ä¿å­˜åˆ°: {error_analysis_path}")
        
        # ä¿å­˜Top-Nåˆ†å¸ƒ
        top_n_dfs = {}
        for top_k, distribution in top_n_distribution.items():
            top_n_df = pd.DataFrame([
                {
                    'item_name': item_name,
                    'count': info['count'],
                    'percentage': info['percentage']
                }
                for item_name, info in distribution.items()
            ]).sort_values('count', ascending=False)
            top_n_dfs[top_k] = top_n_df
            
            top_n_path = os.path.join(output_dir, f"top_{top_k}_distribution.csv")
            top_n_df.to_csv(top_n_path, index=False)
            log_info(f"ğŸ“ˆ {top_k}åˆ†å¸ƒå·²ä¿å­˜åˆ°: {top_n_path}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        results_df = pd.DataFrame({
            "true_label": y_true_labels,
            "predicted_label": y_pred_labels,
            "correct": y_true_labels == y_pred_labels,
        })
        
        # æ·»åŠ æ¦‚ç‡ä¿¡æ¯
        if model_type == 'bert':
            for i, label in enumerate(model_manager.label_encoder.classes_):
                results_df[f"prob_{label}"] = proba[:, i]
        else:
            for i, label in enumerate(label_encoder.classes_):
                results_df[f"prob_{label}"] = y_proba[:, i]
        
        results_path = os.path.join(output_dir, "eval_results.csv")
        results_df.to_csv(results_path, index=False)
        log_info(f"ğŸ¯ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # æ˜¾ç¤ºé”™è¯¯æ ·æœ¬
        if args.show_errors:
            error_df = results_df[results_df["correct"] == False]
            if len(error_df) > 0:
                log_info(f"\nâŒ é”™è¯¯æ ·æœ¬ (å‰10ä¸ª):")
                for idx, row in error_df.head(10).iterrows():
                    log_info(f"  çœŸå®: {row['true_label']}, é¢„æµ‹: {row['predicted_label']}")
        
        # ä½¿ç”¨æ–°çš„æ—¥å¿—è®°å½•åŠŸèƒ½è®°å½•é”™è¯¯åˆ†æ
        log_metrics(error_analysis, "error_analysis")
        
        # è®°å½•Top-5çƒ­é—¨é¡¹ç›®
        if 'top_5' in top_n_distribution:
            top_5_items = {
                item_name: info for item_name, info in
                sorted(top_n_distribution['top_5'].items(),
                      key=lambda x: x[1]['count'], reverse=True)[:5]
            }
            log_metrics(top_5_items, "distribution")
    
    # è®°å½•å®éªŒæ‘˜è¦
    experiment_info = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'model_type': model_type,
        'model_path': model_path,
        'data_info': {
            'eval_samples': len(y_ev),
            'num_classes': len(label_classes)
        },
        'key_metrics': {
            'accuracy': eval_metrics.get('accuracy', 0),
            'hit@1': eval_metrics.get('hit@1', 0),
            'mrr': eval_metrics.get('mrr', 0),
            'ndcg@5': eval_metrics.get('ndcg@5', 0)
        },
        'output_files': {
            'metrics': metrics_path if args.detailed_report else "æœªç”Ÿæˆ",
            'classification_report': report_path if args.detailed_report else "æœªç”Ÿæˆ",
            'confusion_matrix': cm_path if args.detailed_report else "æœªç”Ÿæˆ",
            'error_analysis': error_analysis_path if args.detailed_report else "æœªç”Ÿæˆ"
        }
    }
    
    log_experiment_summary(experiment_info)
    
    total_time = time.time() - global_start
    log_info(f"\nğŸ‰ {model_type.upper()}è¯„ä¼°å®Œæˆï¼")
    log_info(f"â±ï¸  æ€»è€—æ—¶ï¼š{fmt_sec(total_time)}")
    log_info(f"ğŸ“Š è¯„ä¼°æ ·æœ¬æ•°ï¼š{len(y_ev)}")

    # ä½¿ç”¨ Hit@1 ä½œä¸ºâ€œå‡†ç¡®ç‡â€å±•ç¤ºï¼Œé¿å…ç¼ºå°‘ accuracy é”®å¯¼è‡´æŠ¥é”™
    hit1 = eval_metrics.get('hit@1') or eval_metrics.get('hit_1')
    if hit1 is not None:
        log_info(f"ğŸ¯ Hit@1ï¼ˆå‡†ç¡®ç‡ï¼‰ï¼š{hit1:.4f}")
    else:
        log_info("ğŸ¯ æœªèƒ½è·å– Hit@1 æŒ‡æ ‡")
    
    # æ¸…ç†å†…å­˜
    model_manager.clear_memory()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    # æ¨¡å‹ç±»å‹é€‰æ‹© - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    parser.add_argument("--model-type", type=str, default="bert", choices=["bert", "tfidf"], help="æ¨¡å‹ç±»å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--eval-file", type=str, default="eval.csv", help="éªŒè¯é›†æ–‡ä»¶å")
    parser.add_argument("--outdir", type=str, default="./output/2025_up_to_month_2", help="æ•°æ®ç›®å½•")
    parser.add_argument("--experiment-outdir", type=str, default=None, help="å®éªŒè¾“å‡ºç›®å½•")
    parser.add_argument("--modelsdir", type=str, default="./models", help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--model", type=str, required=True, help="æ¨¡å‹æ–‡ä»¶å")
    
    # BERT å‚æ•°
    parser.add_argument("--bert-model", type=str, default="./models/google-bert/bert-base-chinese", help="BERTæ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--eval-batch-size", type=int, default=32)
    parser.add_argument("--max-length", type=int, default=256)
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦è¯„ä¼°")
    parser.add_argument("--allow-online", type=_str2bool, default=False, help="å…è®¸åœ¨çº¿ä¸‹è½½HFæ¨¡å‹")
    
    # æŠ¥å‘Šå‚æ•°
    parser.add_argument("--detailed-report", action="store_true", help="ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    parser.add_argument("--show-errors", action="store_true", help="æ˜¾ç¤ºé”™è¯¯æ ·æœ¬")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--log-level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    
    args = parser.parse_args()
    main(args)