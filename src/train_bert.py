#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨BERTæ¨¡å‹æ›¿ä»£TF-IDFçš„è®­ç»ƒè„šæœ¬ï¼Œå…¼å®¹åŸæœ‰train.pyçš„æ¥å£å’Œè¾“å‡ºæ ¼å¼
"""

import os, json, argparse, warnings, time
import gc
import numpy as np
import pandas as pd
# å¯é€‰ä¾èµ–ï¼šmatplotlib
try:
    import matplotlib.pyplot as plt
    _HAS_MATPLOTLIB = True
except Exception:
    _HAS_MATPLOTLIB = False
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import log_loss, accuracy_score, f1_score
import joblib
from tqdm import tqdm
from contextlib import nullcontext

warnings.filterwarnings("ignore")
from utils import ensure_single_label, build_text, hit_at_k, fmt_sec, _flex_read_csv

# å¯¼å…¥BERTç›¸å…³ç»„ä»¶
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback,
    EarlyStoppingCallback,
)
import torch

# å¯é€‰ä¾èµ–ï¼šimbalanced-learn
try:
    from imblearn.over_sampling import SMOTE, RandomOverSampler
    from imblearn.combine import SMOTEENN, SMOTETomek
    _HAS_IMBLEARN = True
except Exception:
    SMOTE = SMOTEENN = SMOTETomek = RandomOverSampler = None
    _HAS_IMBLEARN = False


def _str2bool(v) -> bool:
    return str(v).lower() in {"1", "true", "t", "y", "yes"}


def _is_valid_local_hf_dir(path: str) -> bool:
    """æ£€æŸ¥æœ¬åœ°Hugging Faceæ¨¡å‹ç›®å½•æ˜¯å¦æœ‰æ•ˆ"""
    if not os.path.isdir(path):
        return False
    needed = [
        os.path.join(path, "config.json"),
    ]
    has_tokenizer = any(
        os.path.exists(os.path.join(path, name))
        for name in ["tokenizer.json", "vocab.txt"]
    )
    if not has_tokenizer:
        return False
    return all(os.path.exists(p) for p in needed)


def _read_split_or_combined(base_dir: str, base_filename: str) -> pd.DataFrame:
    """ä¼˜å…ˆè¯»å– X/Y åˆ†ç¦»æ–‡ä»¶ï¼›è‹¥ä¸å­˜åœ¨åˆ™å›é€€åˆ°å•è¡¨ CSVã€‚

    çº¦å®šï¼š
    - base_filename å¯ä¸º train.csv / eval.csv æˆ– train_X.csv / eval_X.csvï¼›
    - è‹¥ä¸ºå•è¡¨åï¼Œå°†å°è¯•åœ¨ base_dir ä¸‹å¯»æ‰¾ <stem>_X.csv ä¸ <stem>_y.csvï¼›
    - X æ–‡ä»¶åº”åŒ…å«æ–‡æœ¬ç‰¹å¾åˆ—ï¼ˆå¦‚ case_titleã€performed_work ç­‰ï¼‰ï¼Œ
      y æ–‡ä»¶è‡³å°‘åŒ…å« 'linked_items'ï¼ˆè‹¥ä¸º 'label'/'y' ä¼šè‡ªåŠ¨é‡å‘½åï¼‰ã€‚
    """
    base_dir = os.path.abspath(base_dir)
    name = os.path.basename(base_filename)
    stem, ext = os.path.splitext(name)
    # å…¼å®¹ä¼ å…¥ *_X.csv æˆ– *_y.csv çš„æƒ…å†µï¼Œç»Ÿä¸€å›åˆ°å…¬å…± stem
    if stem.endswith("_X"):
        stem = stem[:-2]
    if stem.endswith("_y"):
        stem = stem[:-2]

    x_name = f"{stem}_X.csv"
    y_name = f"{stem}_y.csv"

    def _exists_in_dir(fname: str) -> str | None:
        p = os.path.join(base_dir, fname)
        return p if os.path.exists(p) else None

    # 1) ä¼˜å…ˆå°è¯•åˆ†è¡¨
    x_path = _exists_in_dir(x_name)
    y_path = _exists_in_dir(y_name)
    if x_path and y_path:
        X = _flex_read_csv(base_dir, os.path.basename(x_path))
        y = _flex_read_csv(base_dir, os.path.basename(y_path))

        # å…¼å®¹ y åˆ—å
        if "linked_items" not in y.columns:
            if "label" in y.columns:
                y = y.rename(columns={"label": "linked_items"})
            elif "y" in y.columns:
                y = y.rename(columns={"y": "linked_items"})
            else:
                # è‹¥å¤šåˆ—ï¼Œå–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡ç­¾
                first_label_col = y.columns[0]
                warnings.warn(f"æœªæ‰¾åˆ° 'linked_items'ï¼Œä½¿ç”¨ '{first_label_col}' ä½œä¸ºæ ‡ç­¾åˆ—")
                y = y.rename(columns={first_label_col: "linked_items"})

        # åªä¿ç•™æ ‡ç­¾åˆ—
        y = y[["linked_items"]]
        if len(X) != len(y):
            raise ValueError(f"X/Y è¡Œæ•°ä¸ä¸€è‡´ï¼šX={len(X)} Y={len(y)}ï¼ˆstem={stem}ï¼‰")
        df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)
        return df

    # 2) å›é€€ï¼šè¯»å–å•è¡¨ï¼ˆä¾‹å¦‚ train.csv / eval.csvï¼‰
    warnings.warn(
        f"æœªæ‰¾åˆ°åˆ†è¡¨ {x_name}+{y_name}ï¼Œå›é€€åˆ°å•è¡¨ {name}ï¼ˆç›®å½•ï¼š{base_dir}ï¼‰"
    )
    return _flex_read_csv(base_dir, name)


def _choose_label_column(df: pd.DataFrame) -> str:
    """é€‰æ‹©æ ‡ç­¾åˆ—"""
    # ä¼˜å…ˆçº§ï¼šextend_id > linked_items > item_title
    for col in ["extend_id", "linked_items", "item_title"]:
        if col in df.columns:
            return col
    raise KeyError("æœªæ‰¾åˆ°å¯ç”¨æ ‡ç­¾åˆ—ï¼ˆextend_id/linked_items/item_titleï¼‰")


class BERTDataset(torch.utils.data.Dataset):
    """BERTæ•°æ®é›†ç±»"""
    def __init__(self, encodings: dict, labels: np.ndarray | None = None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx: int):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        if self.labels is not None:
            item["labels"] = torch.tensor(int(self.labels[idx]))
        return item

    def __len__(self) -> int:
        return len(self.encodings["input_ids"])


class LossRecorder(TrainerCallback):
    """è®°å½•è®­ç»ƒæŸå¤±"""
    def __init__(self):
        self.losses: list[float] = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        # logging_strategy='epoch' æ—¶ï¼Œlogs å†…å« 'loss'
        if state.is_world_process_zero and ("loss" in logs):
            try:
                self.losses.append(float(logs["loss"]))
            except Exception:
                pass


def _compute_metrics(eval_pred, num_labels: int) -> dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1w = f1_score(labels, preds, average="weighted")
    f1m = f1_score(labels, preds, average="macro")
    # è®¡ç®— hit@k
    e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    proba = e / e.sum(axis=1, keepdims=True)
    out = {
        "accuracy": float(acc),
        "f1_weighted": float(f1w),
        "f1_macro": float(f1m),
        "hit@1": hit_at_k(labels, proba, 1),
        "hit@3": hit_at_k(labels, proba, 3),
        "hit@5": hit_at_k(labels, proba, 5) if num_labels >= 5 else float("nan"),
        "hit@10": hit_at_k(labels, proba, 10) if num_labels >= 10 else float("nan"),
    }
    return out


class BERTModelWrapper:
    """BERTæ¨¡å‹åŒ…è£…å™¨ï¼Œå…¼å®¹sklearnæ¥å£ï¼Œæ”¯æŒæ‡’åŠ è½½ä¸åˆ†æ‰¹æ¨ç†ä»¥é¿å…æ˜¾å­˜æº¢å‡º"""
    def __init__(self, model_path, tokenizer, label_encoder, device='cpu'):
        self.model_path = model_path
        self.tokenizer = tokenizer
        self.label_encoder = label_encoder
        # ç»Ÿä¸€ä¸º torch.device
        try:
            self.device = torch.device(device) if not isinstance(device, torch.device) else device
        except Exception:
            self.device = torch.device('cpu')
        self.model = None

    def _ensure_model(self):
        """ç¡®ä¿åº•å±‚HFæ¨¡å‹å·²åŠ è½½è‡³ self.modelï¼Œå¹¶ç§»åŠ¨åˆ° self.deviceã€‚"""
        if self.model is None:
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_path,
                local_files_only=True
            )
            self.model.to(self.device)
            self.model.eval()

    def fit(self, X=None, y=None):
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¸ sklearn æ¥å£å¯¹é½ï¼‰ã€‚"""
        self._ensure_model()
        return self

    def predict_proba_batched(self, texts, batch_size: int = 16, max_length: int = 256):
        """åˆ†æ‰¹é¢„æµ‹æ¦‚ç‡ï¼Œè‡ªåŠ¨åœ¨ CUDA/MPS/CPU ä¹‹é—´é€‰æ‹©ï¼Œå¹¶åœ¨ OOM æ—¶å›é€€ã€‚"""
        if isinstance(texts, str):
            texts = [texts]
        # ç¡®ä¿æ¨¡å‹åŠ è½½
        self._ensure_model()
        # è®¾å¤‡ä¸å°è¯•åºåˆ—
        devs = []
        try:
            if torch.cuda.is_available():
                devs.append(torch.device('cuda'))
            if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():
                devs.append(torch.device('mps'))
        except Exception:
            pass
        devs.append(torch.device('cpu'))

        last_err = None
        for dev in devs:
            try:
                # ç§»åŠ¨æ¨¡å‹åˆ°ç›®æ ‡è®¾å¤‡
                try:
                    self.model.to(dev)
                    self.device = dev
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                except Exception:
                    pass
                # é€æ­¥ç¼©å° batch size
                for bs in [batch_size, max(1, batch_size // 2), max(1, batch_size // 4)]:
                    use_amp = (isinstance(dev, torch.device) and dev.type == 'cuda')
                    probs = _predict_proba_in_batches(
                        model=self.model,
                        tokenizer=self.tokenizer,
                        texts=texts,
                        device=dev,
                        max_length=max_length,
                        batch_size=int(bs),
                        use_amp=use_amp,
                    )
                    return probs
            except Exception as e:
                last_err = e
                # OOM æˆ–è®¾å¤‡é”™è¯¯åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€ç§ç»„åˆ
                continue
        # è‹¥æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œåˆ™æŠ›å‡ºæœ€åçš„å¼‚å¸¸
        if last_err is not None:
            raise last_err
        # æç«¯å…œåº•ï¼ˆç†è®ºä¸Šä¸ä¼šåˆ°æ­¤ï¼‰
        return np.zeros((0, 0), dtype=np.float32)

    def predict_proba(self, texts):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆé»˜è®¤èµ°åˆ†æ‰¹æ¨ç†ï¼Œå®‰å…¨ä¸”ç¨³å¥ï¼‰ã€‚"""
        return self.predict_proba_batched(texts, batch_size=16, max_length=256)

    def predict(self, texts):
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(texts)
        return self.label_encoder.inverse_transform(np.argmax(proba, axis=1))


def _predict_proba_in_batches(model, tokenizer, texts, device, max_length, batch_size=16, use_amp=False):
    """åˆ†æ‰¹è®¡ç®—æ–‡æœ¬çš„ç±»åˆ«æ¦‚ç‡ï¼Œé¿å…ä¸€æ¬¡æ€§å æ»¡æ˜¾å­˜"""
    if isinstance(texts, str):
        texts = [texts]
    model.eval()
    all_probs: list[torch.Tensor] = []
    amp_ctx = torch.cuda.amp.autocast(dtype=torch.float16) if (use_amp and isinstance(device, torch.device) and device.type == 'cuda') else nullcontext()
    with torch.inference_mode(), amp_ctx:
        for i in range(0, len(texts), int(batch_size)):
            batch_texts = texts[i:i + int(batch_size)]
            enc = tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=int(max_length),
                return_tensors='pt'
            )
            enc = {k: v.to(device, non_blocking=True) for k, v in enc.items()}
            out = model(**enc)
            probs = torch.softmax(out.logits, dim=-1).to('cpu')
            all_probs.append(probs)
            del enc, out
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
    if not all_probs:
        return np.zeros((0, 0), dtype=np.float32)
    return torch.cat(all_probs, dim=0).numpy()


def main(args):
    global_start = time.time()
    print("=== BERTæ¨¡å‹è®­ç»ƒå¼€å§‹ï¼ˆå…¼å®¹TF-IDFæ¥å£ï¼‰===")

    os.makedirs(args.outdir, exist_ok=True)
    os.makedirs(args.modelsdir, exist_ok=True)

    # è¯»å–æ•°æ®ï¼ˆä½¿ç”¨ä¸train.pyç›¸åŒçš„é€»è¾‘ï¼‰
    print("ğŸ“– æ­£åœ¨è¯»å–è®­ç»ƒæ•°æ®...")
    df_tr = _read_split_or_combined(args.outdir, args.train_file)
    print(f"âœ“ è®­ç»ƒæ•°æ®è¯»å–å®Œæˆ: {df_tr.shape}")
    
    print("ğŸ“– æ­£åœ¨è¯»å–è¯„ä¼°æ•°æ®...")
    df_ev = _read_split_or_combined(args.outdir, args.eval_file)
    print(f"âœ“ è¯„ä¼°æ•°æ®è¯»å–å®Œæˆ: {df_ev.shape}")
    
    label_col = _choose_label_column(df_tr)
    print(f"âœ“ é€‰æ‹©æ ‡ç­¾åˆ—: {label_col}")

    # æ£€æŸ¥å¿…è¦åˆ—
    for df_name, df in [("train", df_tr), ("eval", df_ev)]:
        for col in ["case_title", "performed_work", label_col]:
            if col not in df.columns:
                raise KeyError(f"{df_name}.csv ç¼ºå°‘åˆ—ï¼š{col}")

    # æ¸…æ´—æ ‡ç­¾
    df_tr[label_col] = df_tr[label_col].apply(ensure_single_label).astype(str)
    df_ev[label_col] = df_ev[label_col].apply(ensure_single_label).astype(str)

    X_tr = build_text(df_tr).tolist()
    y_tr_raw = df_tr[label_col].astype(str).tolist()
    X_ev = build_text(df_ev).tolist()
    y_ev_raw = df_ev[label_col].astype(str).tolist()

    # å¦‚æœæŸä¸€æ ‡ç­¾ä¸‹åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œé‚£å°±æŠŠè¿™ä¸ªæ ·æœ¬å¤åˆ¶ä¸€ä»½ï¼ˆæç«¯å°‘æ ·æœ¬çš„å…œåº•ï¼‰
    print("ğŸ” æ£€æŸ¥ç¨€æœ‰æ ‡ç­¾...")
    vc = df_tr[label_col].value_counts()
    rare_labels = vc[vc == 1].index.tolist()
    if rare_labels:
        rare_samples = df_tr[df_tr[label_col].isin(rare_labels)]
        df_tr = pd.concat([df_tr, rare_samples], ignore_index=True)
        print(f"âš ï¸  å·²å¤åˆ¶ {len(rare_samples)} ä¸ªå•æ ·æœ¬ç±»åˆ«ï¼Œä»¥å¹³è¡¡è®­ç»ƒé›†ã€‚")
        # æ›´æ–°æ–‡æœ¬ä¸æ ‡ç­¾ï¼ˆå¤åˆ¶åï¼‰
        X_tr = build_text(df_tr).tolist()
        y_tr_raw = df_tr[label_col].astype(str).tolist()
    else:
        print("âœ“ æ— éœ€å¤åˆ¶ç¨€æœ‰æ ‡ç­¾æ ·æœ¬")

    print("ğŸ·ï¸  æ­£åœ¨ç¼–ç æ ‡ç­¾...")
    le = LabelEncoder()
    y_tr = le.fit_transform(y_tr_raw)
    print(f"âœ“ æ ‡ç­¾ç¼–ç å®Œæˆï¼Œå…± {len(le.classes_)} ä¸ªç±»åˆ«")

    # è¿‡æ»¤ eval ä¸­ä¸åœ¨è®­ç»ƒæ ‡ç­¾é›†çš„æ ·æœ¬
    ev_mask = [lbl in set(le.classes_) for lbl in y_ev_raw]
    if not all(ev_mask):
        dropped = int(np.sum(~np.array(ev_mask)))
        print(f"[è­¦å‘Š] eval ä¸­æœ‰ {dropped} æ¡æ ·æœ¬çš„æ ‡ç­¾æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼ˆè®°ä¸º not_in_trainï¼‰")
    X_ev_f = [t for t, m in zip(X_ev, ev_mask) if m]
    y_ev_f = [l for l, m in zip(y_ev_raw, ev_mask) if m]
    y_ev = le.transform(y_ev_f) if len(y_ev_f) > 0 else np.array([])

    # ========== ç±»åˆ«ä¸å¹³è¡¡å¤„ç† ==========
    resample_method = getattr(args, "resample_method", "none")
    if resample_method != "none":
        print(f"[Info] å¯ç”¨ä¸å¹³è¡¡é‡‡æ ·ï¼š{resample_method}")
        
        # ç®€æ˜“éšæœºè¿‡é‡‡æ ·ï¼ˆæ— ä¾èµ–å›é€€ï¼‰
        def _simple_ros(X, y):
            y = np.asarray(y)
            classes, counts = np.unique(y, return_counts=True)
            max_n = counts.max()
            idx_all = []
            rng = np.random.default_rng(42)
            for c in classes:
                idx_c = np.where(y == c)[0]
                if len(idx_c) == 0:
                    continue
                if len(idx_c) < max_n:
                    extra = rng.choice(idx_c, size=max_n - len(idx_c), replace=True)
                    idx_c = np.concatenate([idx_c, extra], axis=0)
                idx_all.append(idx_c)
            sel = np.concatenate(idx_all, axis=0)
            return [X[i] for i in sel], y[sel]
        
        sampler = None
        
        if resample_method == "smote":
            if _HAS_IMBLEARN and SMOTE is not None:
                sampler = SMOTE(random_state=42)
            else:
                print("[Warning] æœªå®‰è£… imbalanced-learn æˆ–å¯¼å…¥å¤±è´¥ï¼ŒSMOTE å›é€€ä¸ºéšæœºè¿‡é‡‡æ ·ï¼ˆç®€æ˜“å®ç°ï¼‰ã€‚")
                resample_method = "ros"
        
        if resample_method == "smoteenn":
            if _HAS_IMBLEARN and SMOTEENN is not None:
                sampler = SMOTEENN(random_state=42)
            else:
                print("[Warning] æœªå®‰è£… imbalanced-learn æˆ–å¯¼å…¥å¤±è´¥ï¼ŒSMOTEENN å›é€€ä¸ºéšæœºè¿‡é‡‡æ ·ï¼ˆç®€æ˜“å®ç°ï¼‰ã€‚")
                resample_method = "ros"
        
        if resample_method == "smotetomek":
            if _HAS_IMBLEARN and SMOTETomek is not None:
                sampler = SMOTETomek(random_state=42)
            else:
                print("[Warning] æœªå®‰è£… imbalanced-learn æˆ–å¯¼å…¥å¤±è´¥ï¼ŒSMOTETomek å›é€€ä¸ºéšæœºè¿‡é‡‡æ ·ï¼ˆç®€æ˜“å®ç°ï¼‰ã€‚")
                resample_method = "ros"
        
        if resample_method == "ros":
            if _HAS_IMBLEARN and RandomOverSampler is not None:
                sampler = RandomOverSampler(random_state=42)
            else:
                sampler = None  # ä½¿ç”¨ç®€æ˜“ ROS
        
        # æ‰§è¡Œé‡‡æ ·
        if sampler is not None:
            # å¯¹äºæ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç¼–ç å†é‡‡æ ·
            temp_enc = tokenizer(X_tr, padding=False, truncation=True, max_length=512)
            # å°†ç¼–ç è½¬æ¢ä¸ºnumpyæ•°ç»„ç”¨äºé‡‡æ ·
            X_temp = np.array([np.array(ids) for ids in temp_enc["input_ids"]])
            X_temp, y_tr = sampler.fit_resample(X_temp.reshape(len(X_temp), -1), y_tr)
            # é‡å»ºæ–‡æœ¬åˆ—è¡¨ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰
            X_tr = [X_tr[i % len(X_tr)] for i in range(len(y_tr))]
        else:
            X_tr, y_tr = _simple_ros(X_tr, y_tr)
        
        print(f"[Info] é‡‡æ ·åè®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_tr)}")
    # ========== ä¸å¹³è¡¡å¤„ç†ç»“æŸ ==========

    # Tokenizer & Model
    # æ”¯æŒä»æœ¬åœ° models/ ç›®å½•åŠ è½½ï¼šä¼˜å…ˆä½¿ç”¨ --init-hf-dirï¼›å¦åˆ™ä½¿ç”¨ --bert-modelï¼ˆå¯ä¸ºæœ¬åœ°ç›®å½•æˆ–æ¨¡å‹åï¼‰
    print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–BERTæ¨¡å‹...")
    init_path = getattr(args, "init_hf_dir", None) or args.bert_model
    model_name = init_path
    is_local = os.path.isdir(init_path)
    allow_online = bool(getattr(args, "allow_online", False))

    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {init_path}")
    print(f"ğŸŒ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {is_local}")

    if is_local:
        print("ğŸ” éªŒè¯æœ¬åœ°æ¨¡å‹ç›®å½•...")
        if not _is_valid_local_hf_dir(init_path):
            # å°è¯•åœ¨å…¶å­ç›®å½•ä¸­è‡ªåŠ¨å‘ç°ä¸€ä¸ªåˆæ³•çš„HFæ¨¡å‹ç›®å½•ï¼ˆå¸¸è§å¸ƒå±€: ./models/<publisher>/<model_name>ï¼‰
            discovered = None
            try:
                for root, dirs, files in os.walk(init_path):
                    # åªæ·±å…¥ä¸¤å±‚ï¼Œé¿å…æ‰«æè¿‡å¤š
                    depth = root[len(init_path):].count(os.sep)
                    if depth > 3:
                        continue
                    if "config.json" in files and ("tokenizer.json" in files or "vocab.txt" in files):
                        discovered = root
                        break
                if discovered:
                    print(f"[æç¤º] ä¼ å…¥ç›®å½• {init_path} ä¸å«æ¨¡å‹æ–‡ä»¶ï¼Œè‡ªåŠ¨å‘ç°å­ç›®å½•: {discovered}")
                    init_path = discovered  # æ›¿æ¢ä¸ºçœŸå®æ¨¡å‹è·¯å¾„
                else:
                    raise RuntimeError(
                        f"æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å®Œæ•´ï¼š{init_path}\n"
                        f"æœªæ‰¾åˆ°åŒ…å« config.json ä¸ tokenizer.json/vocab.txt çš„å­ç›®å½•ã€‚\n"
                        f"è¯·ä½¿ç”¨ --bert-model æŒ‡å‘å…·ä½“æ¨¡å‹ç›®å½•ï¼Œä¾‹å¦‚: ./models/google-bert/bert-base-chinese"
                    )
            except Exception as e:
                raise RuntimeError(
                    f"æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å®Œæ•´ï¼š{init_path}\nè‡ªåŠ¨å‘ç°å­ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}\n"
                    f"è¯·ç¡®ä¿åŒ…å«è‡³å°‘ config.json ä¸ tokenizer.json æˆ– vocab.txtã€‚"
                )
        print("ğŸ“¥ åŠ è½½æœ¬åœ°åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(init_path, local_files_only=True)
    else:
        if not allow_online:
            raise RuntimeError(
                "æœªæä¾›æœ¬åœ°æ¨¡å‹ç›®å½•ä¸”ç¦ç”¨äº†è”ç½‘ä¸‹è½½ã€‚è¯·ä½¿ç”¨ä»¥ä¸‹å…¶ä¸€ï¼š\n"
                "1) å…ˆå°†æ¨¡å‹ç¦»çº¿ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¹¶é€šè¿‡ --init-hf-dir æŒ‡å‘è¯¥ç›®å½•ï¼›\n"
                "2) è¿è¡Œæ—¶æ·»åŠ  --allow-onlineï¼Œå¹¶å¯è®¾ç½® HF_ENDPOINT=https://hf-mirror.com ä¸æ¸…ç†ä»£ç†ç¯å¢ƒä»¥åŠ é€Ÿä¸é¿å…è§£æå¤±è´¥ã€‚"
            )
        print("ğŸ“¥ ä»åœ¨çº¿åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(init_path, local_files_only=False)
    
    num_labels = len(le.classes_)
    # è®¾å¤‡ä¸æ··ç²¾åº¦ï¼šä»…åœ¨ CUDA å¯ç”¨æ—¶å¯ç”¨ fp16
    device = (
        "cuda" if torch.cuda.is_available() else ("mps" if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available() else "cpu")
    )
    use_fp16 = bool(args.fp16) and device == "cuda"
    
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ·ï¸  æ ‡ç­¾æ•°é‡: {num_labels}")
    print(f"âš¡ æ··åˆç²¾åº¦: {use_fp16}")

    # è‹¥ init_path åˆ†ç±»å¤´ç»´åº¦ä¸å½“å‰ä»»åŠ¡æ ‡ç­¾æ•°ä¸ä¸€è‡´ï¼Œä½¿ç”¨ ignore_mismatched_sizes è‡ªåŠ¨é‡å»ºåˆ†ç±»å¤´
    print("ğŸ—ï¸  æ­£åœ¨åŠ è½½BERTæ¨¡å‹...")
    if is_local:
        model = AutoModelForSequenceClassification.from_pretrained(
            init_path,
            num_labels=num_labels,
            ignore_mismatched_sizes=True,
            local_files_only=True,
        )
    else:
        model = AutoModelForSequenceClassification.from_pretrained(
            init_path,
            num_labels=num_labels,
            ignore_mismatched_sizes=True,
            local_files_only=False,
        )
    
    print("ğŸ“¤ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡...")
    model.to(device)
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

    # ç¼–ç 
    def _tokenize(batch_texts: list[str]):
        return tokenizer(
            batch_texts,
            padding=False,
            truncation=True,
            max_length=int(args.max_length),
        )

    print("ğŸ”¤ æ­£åœ¨ç¼–ç è®­ç»ƒæ•°æ®...")
    enc_tr = _tokenize(X_tr)
    print(f"âœ“ è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆ: {len(enc_tr['input_ids'])} æ ·æœ¬")
    
    if len(X_ev_f) > 0:
        print("ğŸ”¤ æ­£åœ¨ç¼–ç è¯„ä¼°æ•°æ®...")
        enc_ev = _tokenize(X_ev_f)
        print(f"âœ“ è¯„ä¼°æ•°æ®ç¼–ç å®Œæˆ: {len(enc_ev['input_ids'])} æ ·æœ¬")
    else:
        enc_ev = _tokenize(["dummy"])  # ä¿è¯ä¸ä¸ºç©º
        print("âš ï¸  è¯„ä¼°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨è™šæ‹Ÿæ•°æ®")
    
    # ä¿æŒä¸º list-of-idsï¼Œç”± DataCollatorWithPadding åœ¨ batch ç»´åº¦åš padding
    ds_tr = BERTDataset(dict(enc_tr), labels=np.asarray(y_tr))
    ds_ev = (BERTDataset(dict(enc_ev), labels=np.asarray(y_ev)) if len(X_ev_f) > 0 and len(y_ev) > 0 else None)

    run_dir = os.path.join(args.modelsdir, os.path.splitext(os.path.basename(args.outmodel))[0] + "_bert_runs")
    os.makedirs(run_dir, exist_ok=True)
    print(f"ğŸ“ è¿è¡Œç›®å½•: {run_dir}")

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=run_dir,
        per_device_train_batch_size=int(args.train_batch_size),
        per_device_eval_batch_size=int(args.eval_batch_size),
        learning_rate=float(args.learning_rate),
        num_train_epochs=float(args.num_train_epochs),
        weight_decay=float(args.weight_decay),
        eval_strategy="epoch" if ds_ev is not None else "no",  # ä½¿ç”¨eval_strategyè€Œä¸æ˜¯evaluation_strategy
        logging_strategy="epoch",
        save_strategy="epoch" if ds_ev is not None else "no",
        save_total_limit=1,
        report_to=[],
        load_best_model_at_end=bool(ds_ev is not None),
        metric_for_best_model="eval_loss" if ds_ev is not None else None,
        greater_is_better=False,
        remove_unused_columns=False,
        gradient_accumulation_steps=int(args.grad_accum_steps),
        fp16=use_fp16,
    )

    data_collator = DataCollatorWithPadding(tokenizer)
    loss_recorder = LossRecorder()
    
    # æ—©åœå›è°ƒ
    callbacks = [loss_recorder]
    if ds_ev is not None and getattr(args, "early_stopping_patience", 0) > 0:
        callbacks.append(
            EarlyStoppingCallback(early_stopping_patience=int(args.early_stopping_patience))
        )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds_tr,
        eval_dataset=ds_ev,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=(lambda p: _compute_metrics(p, num_labels)) if ds_ev is not None else None,
        callbacks=callbacks,
    )

    # è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_start = time.time()
    
    # æ·»åŠ è®­ç»ƒè¿›åº¦å›è°ƒ
    class TrainingProgressCallback(TrainerCallback):
        def __init__(self):
            super().__init__()
            self.epoch_start_time = None
            self.epochs_times = []
        
        def on_epoch_begin(self, args, state, control, **kwargs):
            current_epoch = int(state.epoch) + 1
            total_epochs = int(args.num_train_epochs)
            self.epoch_start_time = time.time()
            
            # è®¡ç®—å‰©ä½™æ—¶é—´ä¼°ç®—
            if len(self.epochs_times) > 0:
                avg_epoch_time = sum(self.epochs_times) / len(self.epochs_times)
                remaining_epochs = total_epochs - current_epoch + 1
                eta_seconds = avg_epoch_time * remaining_epochs
                eta_str = fmt_sec(eta_seconds)
            else:
                eta_str = "è®¡ç®—ä¸­..."
            
            # åˆ›å»ºè¿›åº¦æ¡
            progress = int((current_epoch - 1) / total_epochs * 30)
            progress_bar = "â–ˆ" * progress + "â–‘" * (30 - progress)
            
            print(f"\nğŸ“Š Epoch {current_epoch}/{total_epochs} [{progress_bar}] ETA: {eta_str}")
            print(f"   å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S', time.localtime(self.epoch_start_time))}")
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            if state.is_world_process_zero and "loss" in logs:
                current_epoch = int(state.epoch) + 1
                total_epochs = int(args.num_train_epochs)
                loss = float(logs["loss"])
                
                # è®¡ç®—å½“å‰epochå†…çš„è¿›åº¦
                if hasattr(state, 'global_step') and hasattr(args, 'max_steps'):
                    # å°è¯•è·å–æ€»æ­¥æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    try:
                        current_step = state.global_step
                        steps_per_epoch = args.max_steps / total_epochs
                        step_in_epoch = current_step % steps_per_epoch
                        epoch_progress = step_in_epoch / steps_per_epoch
                        
                        # åˆ›å»ºæ›´ç»†ç²’åº¦çš„è¿›åº¦æ¡
                        progress = int(epoch_progress * 20)
                        mini_bar = "â–ˆ" * progress + "â–‘" * (20 - progress)
                        
                        print(f"\r   è®­ç»ƒè¿›åº¦: {mini_bar} {epoch_progress*100:.1f}% | æŸå¤±: {loss:.6f}", end="", flush=True)
                    except:
                        print(f"\r   è®­ç»ƒæŸå¤±: {loss:.6f}", end="", flush=True)
                else:
                    print(f"\r   è®­ç»ƒæŸå¤±: {loss:.6f}", end="", flush=True)
        
        def on_epoch_end(self, args, state, control, **kwargs):
            current_epoch = int(state.epoch)
            total_epochs = int(args.num_train_epochs)
            
            # è®°å½•epochæ—¶é—´
            if self.epoch_start_time is not None:
                epoch_time = time.time() - self.epoch_start_time
                self.epochs_times.append(epoch_time)
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            if len(self.epochs_times) > 0:
                avg_epoch_time = sum(self.epochs_times) / len(self.epochs_times)
                remaining_epochs = total_epochs - current_epoch
                eta_seconds = avg_epoch_time * remaining_epochs
                eta_str = fmt_sec(eta_seconds)
            else:
                eta_str = "è®¡ç®—ä¸­..."
            
            print()  # æ¢è¡Œ
            
            # æ˜¾ç¤ºepochæ€»ç»“
            if hasattr(state, 'log_history') and state.log_history:
                # æ‰¾åˆ°å½“å‰epochçš„æ—¥å¿—
                epoch_logs = [log for log in state.log_history if log.get('epoch') == current_epoch - 1]
                if epoch_logs:
                    last_log = epoch_logs[-1]
                    if 'train_loss' in last_log:
                        train_loss = last_log['train_loss']
                        print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
                    if 'eval_loss' in last_log:
                        eval_loss = last_log['eval_loss']
                        print(f"   éªŒè¯æŸå¤±: {eval_loss:.6f}")
                    if 'eval_accuracy' in last_log:
                        eval_acc = last_log['eval_accuracy']
                        print(f"   éªŒè¯å‡†ç¡®ç‡: {eval_acc:.4f}")
                    if 'eval_f1_macro' in last_log:
                        eval_f1 = last_log['eval_f1_macro']
                        print(f"   éªŒè¯F1-macro: {eval_f1:.4f}")
            
            # æ˜¾ç¤ºæ—¶é—´ä¿¡æ¯
            if self.epoch_start_time is not None:
                epoch_time = time.time() - self.epoch_start_time
                print(f"   Epochè€—æ—¶: {fmt_sec(epoch_time)}")
            
            print(f"   å‰©ä½™æ—¶é—´: {eta_str}")
            
            # åˆ›å»ºæ€»ä½“è¿›åº¦æ¡
            overall_progress = int(current_epoch / total_epochs * 30)
            overall_bar = "â–ˆ" * overall_progress + "â–‘" * (30 - overall_progress)
            print(f"   æ€»ä½“è¿›åº¦: [{overall_bar}] {current_epoch}/{total_epochs} ({current_epoch/total_epochs*100:.1f}%)")
    
    # æ·»åŠ è¿›åº¦å›è°ƒåˆ°ç°æœ‰å›è°ƒåˆ—è¡¨
    progress_callback = TrainingProgressCallback()
    trainer.add_callback(progress_callback)
    
    # æ·»åŠ æ‰¹æ¬¡è¿›åº¦å›è°ƒï¼ˆå¯é€‰ï¼‰
    class BatchProgressCallback(TrainerCallback):
        def __init__(self):
            super().__init__()
            self.last_log_time = time.time()
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs is None:
                return
            
            # é™åˆ¶æ—¥å¿—é¢‘ç‡ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            current_time = time.time()
            if current_time - self.last_log_time < 10:  # æ¯10ç§’æœ€å¤šè¾“å‡ºä¸€æ¬¡
                return
            
            if state.is_world_process_zero and "loss" in logs:
                current_epoch = int(state.epoch) + 1
                total_epochs = int(args.num_train_epochs)
                loss = float(logs["loss"])
                
                # æ˜¾ç¤ºç®€åŒ–çš„è¿›åº¦ä¿¡æ¯
                print(f"\rğŸ”„ Epoch {current_epoch}/{total_epochs} | æŸå¤±: {loss:.6f} | æ—¶é—´: {time.strftime('%H:%M:%S')}", end="", flush=True)
                self.last_log_time = current_time
    
    # æ·»åŠ æ‰¹æ¬¡è¿›åº¦å›è°ƒï¼ˆå¯é€‰ï¼Œæ³¨é‡Šæ‰ä»¥é¿å…è¿‡å¤šè¾“å‡ºï¼‰
    # batch_callback = BatchProgressCallback()
    # trainer.add_callback(batch_callback)
    
    trainer.train()
    train_time = time.time() - train_start
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {fmt_sec(train_time)}")
    print(f"ğŸ“Š å¹³å‡æ¯epochæ—¶é—´: {fmt_sec(train_time / int(args.num_train_epochs))}")

    # è¯„ä¼°
    eval_metrics = {}
    if ds_ev is not None:
        print("\nğŸ“Š å¼€å§‹è¯„ä¼°...")
        eval_start = time.time()
        e_out = trainer.evaluate()
        eval_time = time.time() - eval_start
        print(f"âœ“ è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {fmt_sec(eval_time)}")
        
        # ä» e_out è¯»å– compute_metrics çš„æŒ‡æ ‡
        print("\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
        for k in ["accuracy", "f1_weighted", "f1_macro", "hit@1", "hit@3", "hit@5", "hit@10"]:
            if k in e_out:
                eval_metrics[k] = float(e_out[k])
                print(f"  {k}: {eval_metrics[k]:.4f}")
        # å…œåº•ï¼šè‹¥ compute_metrics æœªæ³¨å†Œï¼Œè‡³å°‘è¾“å‡º loss
        if "eval_loss" in e_out:
            eval_metrics["eval_loss"] = float(e_out["eval_loss"])
            print(f"  eval_loss: {eval_metrics['eval_loss']:.6f}")
    else:
        print("âš ï¸  [æç¤º] eval é›†ä¸ºç©ºæˆ–æ— å¯è¯„ä¼°æ ·æœ¬ï¼Œè·³è¿‡è¯„ä¼°ã€‚")

    # è®­ç»ƒé›†/éªŒè¯é›†æ¦‚ç‡ï¼ˆåˆ†æ‰¹ï¼‰ï¼Œç”¨äº OOD MSP é˜ˆå€¼
    id_pmax_for_stats = None
    try:
        if not getattr(args, 'skip_train_stats', False):
            print("\nğŸ“Š æ­£åœ¨è®¡ç®—è®­ç»ƒåç»Ÿè®¡ï¼ˆåˆ†æ‰¹ï¼‰...")
            stats_device = torch.device('cpu') if getattr(args, 'stats_on_cpu', False) else torch.device(device)
            moved_to_cpu = False
            if getattr(args, 'stats_on_cpu', False) and (isinstance(device, str) and device != 'cpu' or isinstance(device, torch.device) and device.type != 'cpu'):
                model.to('cpu')
                moved_to_cpu = True
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            use_amp_stats = bool(getattr(args, 'fp16', False)) and (hasattr(stats_device, 'type') and stats_device.type == 'cuda')
            probs_tr = _predict_proba_in_batches(
                model=model,
                tokenizer=tokenizer,
                texts=X_tr,
                device=stats_device,
                max_length=int(args.max_length),
                batch_size=int(getattr(args, 'post_train_stats_batch_size', 16)),
                use_amp=use_amp_stats,
            )
            id_pmax_for_stats = probs_tr.max(axis=1)
            if moved_to_cpu:
                model.to(device)
        else:
            # é€€åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨ eval çš„åˆ†å¸ƒä¼°è®¡é˜ˆå€¼ï¼›è‹¥ eval ä¸ºç©ºåˆ™ä½¿ç”¨å›ºå®šé˜ˆå€¼
            if len(X_ev_f) > 0:
                print("\nğŸ“Š è·³è¿‡è®­ç»ƒé›†ç»Ÿè®¡ï¼Œæ”¹ç”¨è¯„ä¼°é›†åˆ†å¸ƒä¼°è®¡é˜ˆå€¼ï¼ˆåˆ†æ‰¹ï¼‰...")
                stats_device = torch.device('cpu') if getattr(args, 'stats_on_cpu', False) else torch.device(device)
                moved_to_cpu = False
                if getattr(args, 'stats_on_cpu', False) and (isinstance(device, str) and device != 'cpu' or isinstance(device, torch.device) and device.type != 'cpu'):
                    model.to('cpu')
                    moved_to_cpu = True
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                use_amp_stats = bool(getattr(args, 'fp16', False)) and (hasattr(stats_device, 'type') and stats_device.type == 'cuda')
                probs_ev = _predict_proba_in_batches(
                    model=model,
                    tokenizer=tokenizer,
                    texts=X_ev_f,
                    device=stats_device,
                    max_length=int(args.max_length),
                    batch_size=int(getattr(args, 'post_train_stats_batch_size', 16)),
                    use_amp=use_amp_stats,
                )
                id_pmax_for_stats = probs_ev.max(axis=1)
                if moved_to_cpu:
                    model.to(device)
            else:
                print("\nâš ï¸  è·³è¿‡ç»Ÿè®¡ä¸”è¯„ä¼°é›†ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼ 0.1ã€‚")
                id_pmax_for_stats = np.array([0.1])
    finally:
        gc.collect()
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass

    tau = float(np.percentile(id_pmax_for_stats, getattr(args, "ooc_tau_percentile", 5.0)))
    temperature = float(getattr(args, "ooc_temperature", 20.0))
    ooc_detector = {"kind": "threshold", "tau": tau, "temperature": temperature}

    # ä¿å­˜ loss æ›²çº¿ä¸æ•°æ®
    loss_curve = loss_recorder.losses
    if _HAS_MATPLOTLIB:
        try:
            plt.figure(figsize=(10, 5))
            plt.plot(loss_curve, label="train")
            plt.xlabel("Epoch")
            plt.ylabel("Loss")
            plt.title("BERT Training Loss Curve")
            plt.grid(True)
            plt.legend()
            plt.savefig(os.path.join(args.outdir, "loss_curve.png"))
            plt.close()
        except Exception:
            pass
    else:
        print("[æç¤º] æœªå®‰è£…matplotlibï¼Œè·³è¿‡æŸå¤±æ›²çº¿å›¾ä¿å­˜")
    with open(os.path.join(args.outdir, "loss_data.json"), "w", encoding="utf-8") as f:
        json.dump({"losses": list(map(float, loss_curve))}, f, ensure_ascii=False, indent=2)
    
    # å†™å…¥è®­ç»ƒæ—¥å¿—
    log_dir = os.path.join(os.getcwd(), "log")
    os.makedirs(log_dir, exist_ok=True)
    stem = os.path.splitext(os.path.basename(args.outmodel))[0]
    train_log_path = os.path.join(log_dir, f"{stem}_train.txt")
    try:
        with open(train_log_path, "w", encoding="utf-8") as f:
            for epoch, loss in enumerate(loss_curve, 1):
                msg = f"epoch={epoch}/{len(loss_curve)} loss={loss:.6f}"
                f.write(msg + "\n")
    except Exception:
        pass

    # ä¿å­˜æ¨¡å‹ä¸æ ‡ç­¾ç¼–ç å™¨ï¼ˆå¯è‡ªå®šä¹‰ç›®å½•ï¼‰
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹...")
    default_dir = os.path.join(
        args.modelsdir,
        os.path.splitext(os.path.basename(args.outmodel))[0] + "_bert",
    )
    model_dir = args.save_hf_dir if getattr(args, "save_hf_dir", None) else default_dir
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"ğŸ“ ä¿å­˜æ¨¡å‹åˆ°: {model_dir}")
    model.save_pretrained(model_dir)
    tokenizer.save_pretrained(model_dir)
    print("âœ“ æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜å®Œæˆ")

    # åˆ›å»ºå…¼å®¹sklearnæ¥å£çš„æ¨¡å‹åŒ…è£…å™¨
    print("ğŸ”§ æ­£åœ¨åˆ›å»ºæ¨¡å‹åŒ…è£…å™¨...")
    bert_wrapper = BERTModelWrapper(model_dir, tokenizer, le, device)
    
    # ä¿å­˜æ¨¡å‹bundleï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
    bundle = {
        "model": bert_wrapper,  # ä½¿ç”¨åŒ…è£…å™¨è€Œä¸æ˜¯Pipeline
        "model_type": "bert",
        "model_dir": model_dir,
        "tokenizer": model_name,
        "label_encoder": le,
        "label_col": label_col,
        "ooc_detector": ooc_detector,
    }
    
    bundle_path = os.path.join(args.modelsdir, args.outmodel)
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹bundleåˆ°: {bundle_path}")
    # ä½¿ç”¨ joblib ä¿å­˜ bundleï¼ˆä¸ TF-IDF çš„ä¿å­˜è·¯å¾„å¯¹é½ï¼‰
    joblib.dump(bundle, bundle_path)
    print("âœ“ æ¨¡å‹bundleä¿å­˜å®Œæˆ")

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    if eval_metrics:
        metrics_path = os.path.join(args.outdir, "metrics_eval.csv")
        pd.DataFrame([eval_metrics]).to_csv(metrics_path, index=False)
        print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")

    total_time = time.time() - global_start
    print(f"\nğŸ‰ BERT è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶ï¼š{fmt_sec(total_time)}")
    print(f"ğŸ“ æ¨¡å‹ç›®å½•ï¼š{model_dir}")
    print(f"ğŸ“¦ æ¨¡å‹æ–‡ä»¶ï¼š{bundle_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-file", type=str, default="train.csv", help="è®­ç»ƒé›†æ–‡ä»¶åï¼ˆé»˜è®¤ä» outdir è¯»å–ï¼‰")
    parser.add_argument("--eval-file", type=str, default="eval.csv", help="éªŒè¯é›†æ–‡ä»¶åï¼ˆé»˜è®¤ä» outdir è¯»å–ï¼‰")
    parser.add_argument("--outdir", type=str, default="./output/2025_up_to_month_2", help="è¾“å‡ºç›®å½•ï¼ˆè¯»å–æ•°æ®ä¸ä¿å­˜è®­ç»ƒæ›²çº¿/æŒ‡æ ‡ï¼‰")
    parser.add_argument("--modelsdir", type=str, default="./models", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--outmodel", type=str, default="9.joblib", help="æ¨¡å‹ä¿å­˜æ–‡ä»¶å")
    
    # BERT å‚æ•°
    parser.add_argument("--bert-model", type=str, default="./models", help="BERTæ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--init-hf-dir", type=str, default=None, help="ä»æœ¬åœ° HF ç›®å½•åˆå§‹åŒ–ï¼ˆè¦†ç›– --bert-modelï¼‰ï¼Œæ”¯æŒç»§ç»­å¾®è°ƒ")
    parser.add_argument("--num-train-epochs", dest="num_train_epochs", type=float, default=3.0)
    parser.add_argument("--train-batch-size", type=int, default=16)
    parser.add_argument("--eval-batch-size", type=int, default=32)
    parser.add_argument("--learning-rate", type=float, default=5e-5)
    parser.add_argument("--weight-decay", type=float, default=0.01)
    parser.add_argument("--grad-accum-steps", type=int, default=1)
    parser.add_argument("--max-length", type=int, default=256)
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä»…CUDAï¼‰")
    parser.add_argument("--save-hf-dir", type=str, default=None, help="ä¿å­˜ Hugging Face æ¨¡å‹ä¸åˆ†è¯å™¨çš„ç›®å½•ï¼ˆé»˜è®¤ models/<stem>_bertï¼‰")
    # åœ¨çº¿/ç¦»çº¿
    parser.add_argument("--allow-online", type=_str2bool, default=False, help="å…è®¸åœ¨çº¿ä¸‹è½½HFæ¨¡å‹ï¼ˆTrue/Falseï¼‰")
    
    # æ—©åœå‚æ•°
    parser.add_argument("--early-stopping-patience", type=int, default=3, help="æ—©åœè€å¿ƒå€¼ï¼ˆè‹¥è¿ç»­ N ä¸ª epoch æœªæå‡åˆ™åœæ­¢ï¼‰")
    
    # OOD/MSP
    parser.add_argument("--ooc-tau-percentile", type=float, default=5.0, help="æ—  OOD æ­£æ ·æœ¬æ—¶ï¼Œp_max çš„åˆ†ä½æ•°é˜ˆå€¼ï¼ˆç™¾åˆ†ä½ï¼‰")
    parser.add_argument("--ooc-temperature", type=float, default=20.0, help="å°† (tau - p_max) ç» sigmoid æ˜ å°„ä¸ºæ¦‚ç‡çš„æ¸©åº¦ç³»æ•°")
    # è®­ç»ƒåç»Ÿè®¡
    parser.add_argument("--skip-train-stats", type=_str2bool, default=False, help="è®­ç»ƒåè·³è¿‡å¯¹è®­ç»ƒé›†æ•´è¡¨æ¦‚ç‡/ç»Ÿè®¡çš„è®¡ç®—ï¼ˆTrue/Falseï¼‰")
    parser.add_argument("--post-train-stats-batch-size", type=int, default=16, help="è®­ç»ƒåè®¡ç®—æ¦‚ç‡çš„batch size")
    parser.add_argument("--stats-on-cpu", type=_str2bool, default=False, help="è®­ç»ƒåç»Ÿè®¡é˜¶æ®µåœ¨CPUä¸Šæ‰§è¡Œï¼ˆTrue/Falseï¼‰")
    
    # ä¸å¹³è¡¡å¤„ç†å‚æ•°
    parser.add_argument(
        "--resample-method",
        type=str,
        default="none",
        choices=["none", "ros", "smote", "smoteenn", "smotetomek"],
        help="ä¸å¹³è¡¡å¤„ç†æ–¹æ³•ï¼šnone/ros/smote/smoteenn/smotetomek",
    )
    
    args = parser.parse_args()
    main(args)