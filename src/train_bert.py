#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€çš„è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒBERTå’ŒTF-IDFæ¨¡å‹
ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨å’Œé…ç½®ç®¡ç†ç³»ç»Ÿ
"""

import os
import json
import argparse
import warnings
import time
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import log_loss, accuracy_score, f1_score
import joblib
from tqdm import tqdm
from contextlib import nullcontext

# å¯¼å…¥æˆ‘ä»¬çš„ç®¡ç†ç³»ç»Ÿ
from model_manager import ModelManager
from config_manager import get_config_manager, ConfigManager
from error_handler import get_error_handler, log_info, log_warning, log_error, handle_exception, handle_oom, retry
from utils import ensure_single_label, build_text, hit_at_k, fmt_sec, _flex_read_csv

warnings.filterwarnings("ignore")

# TF-IDFç›¸å…³å¯¼å…¥
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import SGDClassifier
    from sklearn.calibration import CalibratedClassifierCV
    _HAS_SKLEARN = True
except Exception:
    _HAS_SKLEARN = False

# BERTç›¸å…³å¯¼å…¥
try:
    from transformers import (
        AutoTokenizer,
        AutoModelForSequenceClassification,
        Trainer,
        TrainingArguments,
        DataCollatorWithPadding,
        TrainerCallback,
        EarlyStoppingCallback,
    )
    import torch
    _HAS_TRANSFORMERS = True
except Exception:
    _HAS_TRANSFORMERS = False


class UnifiedDataset(torch.utils.data.Dataset):
    """ç»Ÿä¸€æ•°æ®é›†ç±»ï¼Œæ”¯æŒBERTå’ŒTF-IDF"""
    def __init__(self, encodings=None, texts=None, labels=None):
        self.encodings = encodings  # BERTç¼–ç 
        self.texts = texts  # TF-IDFæ–‡æœ¬
        self.labels = labels

    def __getitem__(self, idx: int):
        if self.encodings is not None:
            # BERTæ•°æ®
            item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
            if self.labels is not None:
                item["labels"] = torch.tensor(int(self.labels[idx]))
            return item
        else:
            # TF-IDFæ•°æ®
            item = {"text": self.texts[idx]}
            if self.labels is not None:
                item["label"] = self.labels[idx]
            return item

    def __len__(self) -> int:
        if self.encodings is not None:
            return len(self.encodings["input_ids"])
        else:
            return len(self.texts)


class LossRecorder(TrainerCallback):
    """è®°å½•è®­ç»ƒæŸå¤±"""
    def __init__(self):
        self.losses: list[float] = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        if state.is_world_process_zero and ("loss" in logs):
            try:
                self.losses.append(float(logs["loss"]))
            except Exception:
                pass


def _compute_metrics(eval_pred, num_labels: int) -> dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1w = f1_score(labels, preds, average="weighted")
    f1m = f1_score(labels, preds, average="macro")
    # è®¡ç®— hit@k
    e = np.exp(logits - np.max(logits, axis=1, keepdims=True))
    proba = e / e.sum(axis=1, keepdims=True)
    out = {
        "accuracy": float(acc),
        "f1_weighted": float(f1w),
        "f1_macro": float(f1m),
        "hit@1": hit_at_k(labels, proba, 1),
        "hit@3": hit_at_k(labels, proba, 3),
        "hit@5": hit_at_k(labels, proba, 5) if num_labels >= 5 else float("nan"),
        "hit@10": hit_at_k(labels, proba, 10) if num_labels >= 10 else float("nan"),
    }
    return out


def _str2bool(v) -> bool:
    return str(v).lower() in {"1", "true", "t", "y", "yes"}


def _read_split_or_combined(base_dir: str, base_filename: str) -> pd.DataFrame:
    """ä¼˜å…ˆè¯»å– X/Y åˆ†ç¦»æ–‡ä»¶ï¼›è‹¥ä¸å­˜åœ¨åˆ™å›é€€åˆ°å•è¡¨ CSVã€‚"""
    base_dir = os.path.abspath(base_dir)
    name = os.path.basename(base_filename)
    stem, ext = os.path.splitext(name)
    # å…¼å®¹ä¼ å…¥ *_X.csv æˆ– *_y.csv çš„æƒ…å†µï¼Œç»Ÿä¸€å›åˆ°å…¬å…± stem
    if stem.endswith("_X"):
        stem = stem[:-2]
    if stem.endswith("_y"):
        stem = stem[:-2]

    x_name = f"{stem}_X.csv"
    y_name = f"{stem}_y.csv"

    def _exists_in_dir(fname: str) -> str | None:
        p = os.path.join(base_dir, fname)
        return p if os.path.exists(p) else None

    # 1) ä¼˜å…ˆå°è¯•åˆ†è¡¨
    x_path = _exists_in_dir(x_name)
    y_path = _exists_in_dir(y_name)
    if x_path and y_path:
        X = _flex_read_csv(base_dir, os.path.basename(x_path))
        y = _flex_read_csv(base_dir, os.path.basename(y_name))

        # å…¼å®¹ y åˆ—å
        if "linked_items" not in y.columns:
            if "label" in y.columns:
                y = y.rename(columns={"label": "linked_items"})
            elif "y" in y.columns:
                y = y.rename(columns={"y": "linked_items"})
            else:
                # è‹¥å¤šåˆ—ï¼Œå–ç¬¬ä¸€åˆ—ä½œä¸ºæ ‡ç­¾
                first_label_col = y.columns[0]
                warnings.warn(f"æœªæ‰¾åˆ° 'linked_items'ï¼Œä½¿ç”¨ '{first_label_col}' ä½œä¸ºæ ‡ç­¾åˆ—")
                y = y.rename(columns={first_label_col: "linked_items"})

        # åªä¿ç•™æ ‡ç­¾åˆ—
        y = y[["linked_items"]]
        if len(X) != len(y):
            raise ValueError(f"X/Y è¡Œæ•°ä¸ä¸€è‡´ï¼šX={len(X)} Y={len(y)}ï¼ˆstem={stem}ï¼‰")
        df = pd.concat([X.reset_index(drop=True), y.reset_index(drop=True)], axis=1)
        return df

    # 2) å›é€€ï¼šè¯»å–å•è¡¨ï¼ˆä¾‹å¦‚ train.csv / eval.csvï¼‰
    warnings.warn(
        f"æœªæ‰¾åˆ°åˆ†è¡¨ {x_name}+{y_name}ï¼Œå›é€€åˆ°å•è¡¨ {name}ï¼ˆç›®å½•ï¼š{base_dir}ï¼‰"
    )
    return _flex_read_csv(base_dir, name)


def _choose_label_column(df: pd.DataFrame) -> str:
    """é€‰æ‹©æ ‡ç­¾åˆ—"""
    # ä¼˜å…ˆçº§ï¼šextend_id > linked_items > item_title
    for col in ["extend_id", "linked_items", "item_title"]:
        if col in df.columns:
            return col
    raise KeyError("æœªæ‰¾åˆ°å¯ç”¨æ ‡ç­¾åˆ—ï¼ˆextend_id/linked_items/item_titleï¼‰")


@handle_exception
@retry(max_retries=3, delay=2.0)
def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    global_start = time.time()
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨å’Œé”™è¯¯å¤„ç†å™¨
    config_manager = get_config_manager()
    config_manager.update_from_args(vars(args))
    
    error_handler = get_error_handler(
        log_file=f"./logs/train_{time.strftime('%Y%m%d_%H%M%S')}.log",
        log_level=config_manager.get_system_config().log_level
    )
    
    # éªŒè¯é…ç½®
    if not config_manager.validate_configs():
        raise RuntimeError("é…ç½®éªŒè¯å¤±è´¥")
    
    # è·å–é…ç½®
    bert_config = config_manager.get_bert_config()
    data_config = config_manager.get_data_config()
    
    # ç¡®å®šæ¨¡å‹ç±»å‹ - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    model_type = getattr(args, 'model_type', 'bert')
    
    log_info(f"=== {model_type.upper()}æ¨¡å‹è®­ç»ƒå¼€å§‹ ===")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(data_config.outdir, exist_ok=True)
    os.makedirs(data_config.modelsdir, exist_ok=True)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    model_manager = ModelManager()
    
    # è¯»å–æ•°æ®
    log_info("ğŸ“– æ­£åœ¨è¯»å–è®­ç»ƒæ•°æ®...")
    log_info(f"   æ•°æ®ç›®å½•: {data_config.outdir}")
    log_info(f"   è®­ç»ƒæ–‡ä»¶: {data_config.train_file}")
    df_tr = _read_split_or_combined(data_config.outdir, data_config.train_file)
    log_info(f"âœ“ è®­ç»ƒæ•°æ®è¯»å–å®Œæˆ: {df_tr.shape}")
    
    log_info("ğŸ“– æ­£åœ¨è¯»å–è¯„ä¼°æ•°æ®...")
    log_info(f"   è¯„ä¼°æ–‡ä»¶: {data_config.eval_file}")
    df_ev = _read_split_or_combined(data_config.outdir, data_config.eval_file)
    log_info(f"âœ“ è¯„ä¼°æ•°æ®è¯»å–å®Œæˆ: {df_ev.shape}")
    
    label_col = _choose_label_column(df_tr)
    log_info(f"âœ“ é€‰æ‹©æ ‡ç­¾åˆ—: {label_col}")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    for df_name, df in [("train", df_tr), ("eval", df_ev)]:
        for col in ["case_title", "performed_work", label_col]:
            if col not in df.columns:
                raise KeyError(f"{df_name}.csv ç¼ºå°‘åˆ—ï¼š{col}")
    
    # æ¸…æ´—æ ‡ç­¾
    df_tr[label_col] = df_tr[label_col].apply(ensure_single_label).astype(str)
    df_ev[label_col] = df_ev[label_col].apply(ensure_single_label).astype(str)
    
    X_tr = build_text(df_tr).tolist()
    y_tr_raw = df_tr[label_col].astype(str).tolist()
    X_ev = build_text(df_ev).tolist()
    y_ev_raw = df_ev[label_col].astype(str).tolist()
    
    # å¤„ç†ç¨€æœ‰æ ‡ç­¾
    log_info("ğŸ” æ£€æŸ¥ç¨€æœ‰æ ‡ç­¾...")
    vc = df_tr[label_col].value_counts()
    rare_labels = vc[vc == 1].index.tolist()
    if rare_labels:
        rare_samples = df_tr[df_tr[label_col].isin(rare_labels)]
        df_tr = pd.concat([df_tr, rare_samples], ignore_index=True)
        log_info(f"âš ï¸  å·²å¤åˆ¶ {len(rare_samples)} ä¸ªå•æ ·æœ¬ç±»åˆ«ï¼Œä»¥å¹³è¡¡è®­ç»ƒé›†ã€‚")
        X_tr = build_text(df_tr).tolist()
        y_tr_raw = df_tr[label_col].astype(str).tolist()
    else:
        log_info("âœ“ æ— éœ€å¤åˆ¶ç¨€æœ‰æ ‡ç­¾æ ·æœ¬")
    
    # ç¼–ç æ ‡ç­¾
    log_info("ğŸ·ï¸  æ­£åœ¨ç¼–ç æ ‡ç­¾...")
    le = LabelEncoder()
    y_tr = le.fit_transform(y_tr_raw)
    log_info(f"âœ“ æ ‡ç­¾ç¼–ç å®Œæˆï¼Œå…± {len(le.classes_)} ä¸ªç±»åˆ«")
    
    # è®¾ç½®æ¨¡å‹ç®¡ç†å™¨çš„æ ‡ç­¾ç¼–ç å™¨
    model_manager.setup_label_encoder(le.classes_.tolist())
    
    # è¿‡æ»¤ eval ä¸­ä¸åœ¨è®­ç»ƒæ ‡ç­¾é›†çš„æ ·æœ¬
    ev_mask = [lbl in set(le.classes_) for lbl in y_ev_raw]
    if not all(ev_mask):
        dropped = int(np.sum(~np.array(ev_mask)))
        log_info(f"[è­¦å‘Š] eval ä¸­æœ‰ {dropped} æ¡æ ·æœ¬çš„æ ‡ç­¾æœªåœ¨è®­ç»ƒé›†ä¸­å‡ºç°ï¼ˆè®°ä¸º not_in_trainï¼‰")
    X_ev_f = [t for t, m in zip(X_ev, ev_mask) if m]
    y_ev_f = [l for l, m in zip(y_ev_raw, ev_mask) if m]
    y_ev = le.transform(y_ev_f) if len(y_ev_f) > 0 else np.array([])
    
    if model_type == 'bert':
        # BERTæ¨¡å‹è®­ç»ƒ
        if not _HAS_TRANSFORMERS:
            raise RuntimeError("BERTæ¨¡å‹è®­ç»ƒéœ€è¦å®‰è£…transformersåº“")
        
        # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
        init_path = bert_config.init_hf_dir or bert_config.model_path
        is_local = os.path.isdir(init_path)
        
        log_info(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {init_path}")
        log_info(f"ğŸŒ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {is_local}")
        log_info(f"ğŸ”— å…è®¸åœ¨çº¿ä¸‹è½½: {bert_config.allow_online}")
        
        # éªŒè¯æœ¬åœ°æ¨¡å‹
        if is_local:
            needed_files = ["config.json"]
            has_tokenizer = any(
                os.path.exists(os.path.join(init_path, name))
                for name in ["tokenizer.json", "vocab.txt"]
            )
            if not has_tokenizer:
                raise RuntimeError(f"æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å®Œæ•´ï¼š{init_path}")
        
        # è®¾ç½®åˆ†è¯å™¨å’Œæ¨¡å‹
        model_manager.setup_tokenizer(init_path, local_files_only=is_local)
        model_manager.setup_model(init_path, len(le.classes_), local_files_only=is_local)

        # å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼ˆéœ€åœ¨ tokenizer åˆå§‹åŒ–åï¼‰
        if bert_config.resample_method != "none":
            X_tr, y_tr = model_manager.handle_imbalanced_data(
                X_tr, y_tr, bert_config.resample_method
            )
        
        # ç¼–ç æ•°æ®
        def _tokenize(batch_texts: list[str]):
            return model_manager.tokenizer(
                batch_texts,
                padding=False,
                truncation=True,
                max_length=bert_config.max_length,
            )
        
        log_info("ğŸ”¤ æ­£åœ¨ç¼–ç è®­ç»ƒæ•°æ®...")
        log_info(f"   æœ€å¤§åºåˆ—é•¿åº¦: {bert_config.max_length}")
        enc_tr = _tokenize(X_tr)
        log_info(f"âœ“ è®­ç»ƒæ•°æ®ç¼–ç å®Œæˆ: {len(enc_tr['input_ids'])} æ ·æœ¬")
        
        if len(X_ev_f) > 0:
            log_info("ğŸ”¤ æ­£åœ¨ç¼–ç è¯„ä¼°æ•°æ®...")
            enc_ev = _tokenize(X_ev_f)
            log_info(f"âœ“ è¯„ä¼°æ•°æ®ç¼–ç å®Œæˆ: {len(enc_ev['input_ids'])} æ ·æœ¬")
        else:
            enc_ev = _tokenize(["dummy"])
            log_warning("âš ï¸  è¯„ä¼°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨è™šæ‹Ÿæ•°æ®")
        
        # åˆ›å»ºæ•°æ®é›†ï¼Œç¡®ä¿æ ‡ç­¾ä¸ºæ•´æ•°ç±»å‹
        ds_tr = UnifiedDataset(encodings=dict(enc_tr), labels=np.asarray(y_tr, dtype=np.int64))
        ds_ev = UnifiedDataset(encodings=dict(enc_ev), labels=np.asarray(y_ev, dtype=np.int64) if len(X_ev_f) > 0 and len(y_ev) > 0 else None)
        
        # è®¾ç½®è¿è¡Œç›®å½•
        if data_config.checkpoint_dir:
            run_dir = os.path.join(data_config.checkpoint_dir, "runs")
            os.makedirs(run_dir, exist_ok=True)
            log_info(f"ğŸ“ è¿è¡Œç›®å½•: {run_dir}")
        else:
            run_dir = os.path.join(data_config.modelsdir, os.path.splitext(os.path.basename(data_config.outmodel))[0] + "_runs")
            os.makedirs(run_dir, exist_ok=True)
            log_info(f"ğŸ“ è¿è¡Œç›®å½•: {run_dir}")
        
        # è®­ç»ƒå‚æ•°
        use_fp16 = bert_config.fp16 and model_manager.device.type == 'cuda'
        
        training_args = TrainingArguments(
            output_dir=run_dir,
            per_device_train_batch_size=bert_config.train_batch_size,
            per_device_eval_batch_size=bert_config.eval_batch_size,
            learning_rate=bert_config.learning_rate,
            num_train_epochs=bert_config.num_train_epochs,
            weight_decay=bert_config.weight_decay,
            eval_strategy="epoch" if ds_ev is not None else "no",
            logging_strategy="epoch",
            save_strategy="epoch" if ds_ev is not None else "no",
            save_total_limit=1,
            report_to=[],
            load_best_model_at_end=bool(ds_ev is not None),
            metric_for_best_model="eval_loss" if ds_ev is not None else None,
            greater_is_better=False,
            remove_unused_columns=False,
            gradient_accumulation_steps=bert_config.grad_accum_steps,
            fp16=use_fp16,
            lr_scheduler_type=bert_config.lr_scheduler_type,
            warmup_ratio=bert_config.warmup_ratio,
            warmup_steps=bert_config.warmup_steps,
        )
        
        data_collator = DataCollatorWithPadding(model_manager.tokenizer)
        loss_recorder = LossRecorder()
        
        # æ—©åœå›è°ƒ
        callbacks = [loss_recorder]
        if ds_ev is not None and bert_config.early_stopping_patience > 0:
            callbacks.append(
                EarlyStoppingCallback(early_stopping_patience=bert_config.early_stopping_patience)
            )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model_manager.model,
            args=training_args,
            train_dataset=ds_tr,
            eval_dataset=ds_ev,
            tokenizer=model_manager.tokenizer,
            data_collator=data_collator,
            compute_metrics=(lambda p: _compute_metrics(p, len(le.classes_))) if ds_ev is not None else None,
            callbacks=callbacks,
        )
        
        # è®­ç»ƒ
        log_info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        train_start = time.time()
        trainer.train()
        train_time = time.time() - train_start
        log_info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        log_info(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {fmt_sec(train_time)}")
        
        # è¯„ä¼°
        eval_metrics = {}
        if ds_ev is not None:
            log_info("\nğŸ“Š å¼€å§‹è¯„ä¼°...")
            eval_start = time.time()
            e_out = trainer.evaluate()
            eval_time = time.time() - eval_start
            log_info(f"âœ“ è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {fmt_sec(eval_time)}")
            
            # ä» e_out è¯»å– compute_metrics çš„æŒ‡æ ‡
            log_info("\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
            for k in ["accuracy", "f1_weighted", "f1_macro", "hit@1", "hit@3", "hit@5", "hit@10"]:
                if k in e_out:
                    eval_metrics[k] = float(e_out[k])
                    log_info(f"  {k}: {eval_metrics[k]:.4f}")
            if "eval_loss" in e_out:
                eval_metrics["eval_loss"] = float(e_out["eval_loss"])
                log_info(f"  eval_loss: {eval_metrics['eval_loss']:.6f}")
        
        # ä¿å­˜æ¨¡å‹
        log_info("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹...")
        
        if data_config.checkpoint_dir:
            model_dir = os.path.join(data_config.checkpoint_dir, "model")
            os.makedirs(model_dir, exist_ok=True)
            log_info(f"ğŸ“ ä¿å­˜æ¨¡å‹åˆ°: {model_dir}")
            model_manager.save_model(model_dir)
        else:
            default_dir = os.path.join(
                data_config.modelsdir,
                os.path.splitext(os.path.basename(data_config.outmodel))[0] + "_model",
            )
            model_dir = bert_config.save_hf_dir or default_dir
            os.makedirs(model_dir, exist_ok=True)
            log_info(f"ğŸ“ ä¿å­˜æ¨¡å‹åˆ°: {model_dir}")
            model_manager.save_model(model_dir)
        
        # ä¿å­˜æ¨¡å‹bundle
        model_manager.save_model_bundle(
            os.path.join(data_config.checkpoint_dir or data_config.modelsdir, data_config.outmodel),
            model_dir,
            model_type="bert",
            label_col=label_col,
            max_length=bert_config.max_length,
            fp16=bert_config.fp16,
        )
        
        # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
        if eval_metrics:
            output_dir = data_config.experiment_outdir or data_config.outdir
            metrics_path = os.path.join(output_dir, "metrics_eval.csv")
            pd.DataFrame([eval_metrics]).to_csv(metrics_path, index=False)
            log_info(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
        
        total_time = time.time() - global_start
        log_info(f"\nğŸ‰ {model_type.upper()} è®­ç»ƒå®Œæˆï¼")
        log_info(f"â±ï¸  æ€»è€—æ—¶ï¼š{fmt_sec(total_time)}")
        log_info(f"ğŸ“ æ¨¡å‹ç›®å½•ï¼š{model_dir}")
        log_info(f"ğŸ“¦ æ¨¡å‹æ–‡ä»¶ï¼š{os.path.join(data_config.checkpoint_dir or data_config.modelsdir, data_config.outmodel)}")
        
    else:
        # TF-IDFæ¨¡å‹è®­ç»ƒ
        if not _HAS_SKLEARN:
            raise RuntimeError("TF-IDFæ¨¡å‹è®­ç»ƒéœ€è¦å®‰è£…scikit-learnåº“")
        
        # åˆ›å»ºTF-IDFç‰¹å¾æå–å™¨
        log_info("ğŸ”¤ åˆ›å»ºTF-IDFç‰¹å¾æå–å™¨...")
        vectorizer = TfidfVectorizer(
            analyzer=getattr(args, 'tfidf_analyzer', 'char_wb'),
            ngram_range=(getattr(args, 'tfidf_ngram_min', 2), getattr(args, 'tfidf_ngram_max', 4)),
            max_features=getattr(args, 'tfidf_max_features', 100000),
            min_df=1,
            sublinear_tf=True
        )
        
        # è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        X_train_vec = vectorizer.fit_transform(X_tr)
        X_test_vec = vectorizer.transform(X_ev_f) if len(X_ev_f) > 0 else None
        y_test = y_ev if len(y_ev_f) > 0 else None
        
        # åˆ›å»ºåˆ†ç±»å™¨
        log_info("ğŸ¤– åˆ›å»ºåˆ†ç±»å™¨...")
        classifier = SGDClassifier(
            loss=getattr(args, 'loss', 'hinge'),
            penalty=getattr(args, 'penalty', 'l2'),
            alpha=getattr(args, 'alpha', 0.0001),
            max_iter=getattr(args, 'max_iter', 100),
            random_state=42,
            learning_rate='adaptive',
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=5,
            tol=1e-3
        )
        
        # è®­ç»ƒ
        log_info("ğŸš€ å¼€å§‹è®­ç»ƒTF-IDFæ¨¡å‹...")
        train_start = time.time()
        classifier.fit(X_train_vec, y_tr)
        train_time = time.time() - train_start
        log_info(f"\nğŸ‰ TF-IDFè®­ç»ƒå®Œæˆï¼")
        log_info(f"â±ï¸  è®­ç»ƒæ—¶é—´: {fmt_sec(train_time)}")
        
        # è¯„ä¼°
        eval_metrics = {}
        if X_test_vec is not None and y_test is not None:
            log_info("\nğŸ“Š å¼€å§‹è¯„ä¼°TF-IDFæ¨¡å‹...")
            eval_start = time.time()
            
            # é¢„æµ‹
            y_pred = classifier.predict(X_test_vec)
            y_proba = classifier.decision_function(X_test_vec)
            
            # è®¡ç®—æ¦‚ç‡
            if y_proba.ndim == 1:
                e = np.exp(y_proba - np.max(y_proba))
                y_proba = e / e.sum(axis=1, keepdims=True)
            else:
                y_proba = np.exp(y_proba - np.max(y_proba, axis=1, keepdims=True))
                y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)
            
            eval_time = time.time() - eval_start
            log_info(f"âœ“ è¯„ä¼°å®Œæˆï¼Œè€—æ—¶ {fmt_sec(eval_time)}")
            
            # è®¡ç®—æŒ‡æ ‡
            acc = accuracy_score(y_test, y_pred)
            f1w = f1_score(y_test, y_pred, average="weighted")
            f1m = f1_score(y_test, y_pred, average="macro")
            
            eval_metrics = {
                "accuracy": float(acc),
                "f1_weighted": float(f1w),
                "f1_macro": float(f1m),
                "hit@1": hit_at_k(y_test, y_proba, 1),
                "hit@3": hit_at_k(y_test, y_proba, 3),
                "hit@5": hit_at_k(y_test, y_proba, 5),
                "hit@10": hit_at_k(y_test, y_proba, 10),
            }
            
            log_info("\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
            for k, v in eval_metrics.items():
                log_info(f"  {k}: {v:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        log_info("\nğŸ’¾ æ­£åœ¨ä¿å­˜TF-IDFæ¨¡å‹...")
        
        # ä¿å­˜æ¨¡å‹bundle
        model_bundle = {
            "model": classifier,
            "vectorizer": vectorizer,
            "label_encoder": le,
            "model_type": "tfidf",
            "label_col": label_col,
        }
        
        bundle_path = os.path.join(data_config.modelsdir, data_config.outmodel)
        joblib.dump(model_bundle, bundle_path)
        log_info(f"âœ“ TF-IDFæ¨¡å‹å·²ä¿å­˜åˆ°: {bundle_path}")
        
        total_time = time.time() - global_start
        log_info(f"\nğŸ‰ {model_type.upper()} è®­ç»ƒå®Œæˆï¼")
        log_info(f"â±ï¸  æ€»è€—æ—¶ï¼š{fmt_sec(total_time)}")
        log_info(f"ğŸ“¦ æ¨¡å‹æ–‡ä»¶ï¼š{bundle_path}")
    
    # æ¸…ç†å†…å­˜
    model_manager.clear_memory()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    
    # æ¨¡å‹ç±»å‹é€‰æ‹© - é»˜è®¤ä¸ºBERTä»¥ä¿æŒå‘åå…¼å®¹æ€§
    parser.add_argument("--model-type", type=str, default="bert", choices=["bert", "tfidf"], help="æ¨¡å‹ç±»å‹")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--train-file", type=str, default="train.csv", help="è®­ç»ƒé›†æ–‡ä»¶å")
    parser.add_argument("--eval-file", type=str, default="eval.csv", help="éªŒè¯é›†æ–‡ä»¶å")
    parser.add_argument("--outdir", type=str, default="./output/2025_up_to_month_2", help="æ•°æ®ç›®å½•")
    parser.add_argument("--experiment-outdir", type=str, default=None, help="å®éªŒè¾“å‡ºç›®å½•")
    parser.add_argument("--modelsdir", type=str, default="./models", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--checkpoint-dir", type=str, default=None, help="checkpointç›®å½•")
    parser.add_argument("--outmodel", type=str, default="bert_model.joblib", help="æ¨¡å‹ä¿å­˜æ–‡ä»¶å")
    
    # BERT å‚æ•°
    parser.add_argument("--bert-model", type=str, default="./models/google-bert/bert-base-chinese", help="BERTæ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--init-hf-dir", type=str, default=None, help="ä»æœ¬åœ° HF ç›®å½•åˆå§‹åŒ–")
    parser.add_argument("--num-train-epochs", dest="num_train_epochs", type=float, default=3.0)
    parser.add_argument("--train-batch-size", type=int, default=16)
    parser.add_argument("--eval-batch-size", type=int, default=32)
    parser.add_argument("--learning-rate", type=float, default=5e-5)
    parser.add_argument("--weight-decay", type=float, default=0.01)
    parser.add_argument("--grad-accum-steps", type=int, default=1)
    parser.add_argument("--max-length", type=int, default=256)
    parser.add_argument("--fp16", action="store_true", help="å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--save-hf-dir", type=str, default=None, help="ä¿å­˜ Hugging Face æ¨¡å‹ä¸åˆ†è¯å™¨çš„ç›®å½•")
    parser.add_argument("--allow-online", type=_str2bool, default=False, help="å…è®¸åœ¨çº¿ä¸‹è½½HFæ¨¡å‹")
    parser.add_argument("--early-stopping-patience", type=int, default=3, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--lr-scheduler-type", type=str, default="cosine",
                       choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"])
    parser.add_argument("--warmup-ratio", type=float, default=0.1)
    parser.add_argument("--warmup-steps", type=int, default=0)
    parser.add_argument("--ooc-tau-percentile", type=float, default=5.0)
    parser.add_argument("--ooc-temperature", type=float, default=20.0)
    parser.add_argument("--skip-train-stats", type=_str2bool, default=False)
    parser.add_argument("--post-train-stats-batch-size", type=int, default=16)
    parser.add_argument("--stats-on-cpu", type=_str2bool, default=False)
    parser.add_argument("--resample-method", type=str, default="none",
                       choices=["none", "ros", "smote", "smoteenn", "smotetomek"])
    
    # TF-IDF å‚æ•°
    parser.add_argument("--tfidf-analyzer", type=str, default="char_wb", choices=["char", "char_wb", "word"])
    parser.add_argument("--tfidf-ngram-min", type=int, default=2)
    parser.add_argument("--tfidf-ngram-max", type=int, default=4)
    parser.add_argument("--tfidf-max-features", type=int, default=100000)
    parser.add_argument("--loss", type=str, default="hinge", choices=["hinge", "log", "modified_huber", "squared_hinge", "perceptron"])
    parser.add_argument("--penalty", type=str, default="l2", choices=["l1", "l2", "elasticnet"])
    parser.add_argument("--alpha", type=float, default=0.0001)
    parser.add_argument("--max-iter", type=int, default=100)
    parser.add_argument("--calibrate", type=str, default="none", choices=["none", "sigmoid", "isotonic"])
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--log-level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"])
    
    args = parser.parse_args()
    main(args)