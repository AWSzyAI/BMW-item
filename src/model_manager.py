#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨ï¼Œå¤„ç†æ¨¡å‹åŠ è½½ã€è®¾å¤‡ç®¡ç†ã€å†…å­˜æ¸…ç†ç­‰
"""

import os
import gc
import warnings
from typing import Dict, Any, Optional, Union, List
import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.preprocessing import LabelEncoder
import joblib

# å¯é€‰ä¾èµ–å¤„ç†
try:
    from imblearn.over_sampling import SMOTE, RandomOverSampler
    from imblearn.combine import SMOTEENN, SMOTETomek
    _HAS_IMBLEARN = True
except Exception:
    SMOTE = SMOTEENN = SMOTETomek = RandomOverSampler = None
    _HAS_IMBLEARN = False

# å¯é€‰ä¾èµ–ï¼šmatplotlib
try:
    import matplotlib.pyplot as plt
    _HAS_MATPLOTLIB = True
except Exception:
    _HAS_MATPLOTLIB = False

from utils import ensure_single_label, build_text, hit_at_k, fmt_sec, _flex_read_csv


class ModelManager:
    """ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨ï¼Œå¤„ç†æ¨¡å‹åŠ è½½ã€è®¾å¤‡ç®¡ç†ã€å†…å­˜æ¸…ç†ç­‰"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹è·¯å¾„ã€è®¾å¤‡é…ç½®ç­‰
        """
        self.config = config or {}
        self.device = self._detect_device()
        self.tokenizer = None
        self.model = None
        self.label_encoder = None
        self.ooc_detector = None
        
    def _detect_device(self) -> torch.device:
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        try:
            if torch.cuda.is_available():
                device = torch.device('cuda')
                print(f"âœ“ æ£€æµ‹åˆ°CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
            elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():
                device = torch.device('mps')
                print("âœ“ æ£€æµ‹åˆ°MPSè®¾å¤‡")
            else:
                device = torch.device('cpu')
                print("âœ“ ä½¿ç”¨CPUè®¾å¤‡")
            return device
        except Exception as e:
            print(f"âš ï¸ è®¾å¤‡æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
            return torch.device('cpu')
    
    def setup_tokenizer(self, model_path: str, local_files_only: bool = True) -> None:
        """
        è®¾ç½®åˆ†è¯å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            local_files_only: æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        """
        try:
            print(f"ğŸ“¥ æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, 
                local_files_only=local_files_only
            )
            print(f"âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        except Exception as e:
            raise RuntimeError(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
    
    def setup_model(self, model_path: str, num_labels: int, local_files_only: bool = True) -> None:
        """
        è®¾ç½®æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            num_labels: åˆ†ç±»æ•°é‡
            local_files_only: æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        """
        try:
            print(f"ğŸ—ï¸ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_path,
                num_labels=num_labels,
                ignore_mismatched_sizes=True,
                local_files_only=local_files_only
            )
            self.model.to(self.device)
            print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def setup_label_encoder(self, labels: List[str]) -> None:
        """
        è®¾ç½®æ ‡ç­¾ç¼–ç å™¨
        
        Args:
            labels: æ ‡ç­¾åˆ—è¡¨
        """
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(labels)
        print(f"âœ“ æ ‡ç­¾ç¼–ç å™¨è®¾ç½®å®Œæˆï¼Œå…± {len(self.label_encoder.classes_)} ä¸ªç±»åˆ«")
    
    def clear_memory(self) -> None:
        """æ¸…ç†å†…å­˜"""
        try:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("âœ“ GPUå†…å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"âš ï¸ å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def switch_device(self, target_device: Union[str, torch.device]) -> bool:
        """
        åˆ‡æ¢è®¾å¤‡
        
        Args:
            target_device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            bool: åˆ‡æ¢æ˜¯å¦æˆåŠŸ
        """
        try:
            if isinstance(target_device, str):
                target_device = torch.device(target_device)
            
            if self.model is not None:
                self.model.to(target_device)
                self.device = target_device
                self.clear_memory()
                print(f"âœ“ è®¾å¤‡å·²åˆ‡æ¢åˆ°: {target_device}")
                return True
            return False
        except Exception as e:
            print(f"âš ï¸ è®¾å¤‡åˆ‡æ¢å¤±è´¥: {e}")
            return False
    
    def predict_proba_batched(self, texts: List[str], batch_size: int = 16, 
                           max_length: int = 256, use_amp: bool = False) -> np.ndarray:
        """
        åˆ†æ‰¹é¢„æµ‹æ¦‚ç‡ï¼Œè‡ªåŠ¨å¤„ç†OOMé”™è¯¯
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            
        Returns:
            np.ndarray: é¢„æµ‹æ¦‚ç‡
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåˆå§‹åŒ–")
        
        if isinstance(texts, str):
            texts = [texts]
        
        # è®¾å¤‡å°è¯•åºåˆ—
        devices = []
        try:
            if torch.cuda.is_available():
                devices.append(torch.device('cuda'))
            if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():
                devices.append(torch.device('mps'))
        except Exception:
            pass
        devices.append(torch.device('cpu'))
        
        last_err = None
        for device in devices:
            try:
                # åˆ‡æ¢åˆ°ç›®æ ‡è®¾å¤‡
                if not self.switch_device(device):
                    continue
                
                # é€æ­¥ç¼©å°æ‰¹æ¬¡å¤§å°
                for bs in [batch_size, max(1, batch_size // 2), max(1, batch_size // 4)]:
                    try:
                        use_amp_current = use_amp and device.type == 'cuda'
                        return self._predict_proba_in_batches(
                            texts, bs, max_length, use_amp_current
                        )
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            print(f"âš ï¸ OOMé”™è¯¯ï¼Œæ‰¹æ¬¡å¤§å° {bs} -> {max(1, bs // 2)}")
                            self.clear_memory()
                            continue
                        raise e
            except Exception as e:
                last_err = e
                continue
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        if last_err is not None:
            raise last_err
        return np.zeros((0, 0), dtype=np.float32)
    
    def _predict_proba_in_batches(self, texts: List[str], batch_size: int, 
                               max_length: int, use_amp: bool = False) -> np.ndarray:
        """
        å†…éƒ¨åˆ†æ‰¹é¢„æµ‹æ–¹æ³•
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            
        Returns:
            np.ndarray: é¢„æµ‹æ¦‚ç‡
        """
        self.model.eval()
        all_probs = []
        
        # è®¾ç½®æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        amp_ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else torch.no_grad()
        
        with torch.inference_mode(), amp_ctx:
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # åˆ†è¯
                enc = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=max_length,
                    return_tensors='pt'
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                enc = {k: v.to(self.device, non_blocking=True) for k, v in enc.items()}
                
                # é¢„æµ‹
                with torch.cuda.amp.autocast(enabled=use_amp):
                    outputs = self.model(**enc)
                    probs = torch.softmax(outputs.logits, dim=-1).to('cpu')
                
                all_probs.append(probs)
                
                # æ¸…ç†ä¸­é—´å˜é‡
                del enc, outputs
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        if not all_probs:
            return np.zeros((0, 0), dtype=np.float32)
        
        return torch.cat(all_probs, dim=0).numpy()
    
    def save_model(self, save_dir: str) -> None:
        """
        ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåˆå§‹åŒ–")
        
        os.makedirs(save_dir, exist_ok=True)
        
        try:
            self.model.save_pretrained(save_dir)
            self.tokenizer.save_pretrained(save_dir)
            print(f"âœ“ æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_dir}")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def load_model_bundle(self, bundle_path: str) -> Dict[str, Any]:
        """
        åŠ è½½æ¨¡å‹bundle
        
        Args:
            bundle_path: bundleæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: åŒ…å«æ¨¡å‹ã€æ ‡ç­¾ç¼–ç å™¨ç­‰çš„bundle
        """
        try:
            bundle = joblib.load(bundle_path)
            print(f"âœ“ æ¨¡å‹bundleåŠ è½½æˆåŠŸ: {bundle_path}")
            return bundle
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹bundleåŠ è½½å¤±è´¥: {e}")
    
    def save_model_bundle(self, bundle_path: str, model_dir: str, 
                        model_type: str = "bert", **kwargs) -> None:
        """
        ä¿å­˜æ¨¡å‹bundle
        
        Args:
            bundle_path: bundleä¿å­˜è·¯å¾„
            model_dir: æ¨¡å‹ç›®å½•
            model_type: æ¨¡å‹ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°
        """
        labels = kwargs.get("labels")
        if labels is None and self.label_encoder is not None:
            labels = self.label_encoder.classes_.tolist()

        bundle = {
            "model_type": model_type,
            "model_dir": model_dir,
            "label_encoder": self.label_encoder,
            "labels": labels,
            "ooc_detector": self.ooc_detector,
        }

        if model_type == "bert":
            bundle.update({
                "max_length": kwargs.get("max_length"),
                "fp16": kwargs.get("fp16", False),
            })
        else:
            bundle.update(kwargs)
        
        try:
            os.makedirs(os.path.dirname(bundle_path), exist_ok=True)
            joblib.dump(bundle, bundle_path)
            print(f"âœ“ æ¨¡å‹bundleå·²ä¿å­˜åˆ°: {bundle_path}")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹bundleä¿å­˜å¤±è´¥: {e}")
    
    def handle_imbalanced_data(self, X: List[str], y: np.ndarray, 
                           method: str = "none") -> tuple[List[str], np.ndarray]:
        """
        å¤„ç†ä¸å¹³è¡¡æ•°æ®
        
        Args:
            X: æ–‡æœ¬åˆ—è¡¨
            y: æ ‡ç­¾æ•°ç»„
            method: é‡‡æ ·æ–¹æ³•
            
        Returns:
            tuple: å¤„ç†åçš„æ–‡æœ¬å’Œæ ‡ç­¾
        """
        if method == "none" or not X:
            return X, y
        
        print(f"ğŸ”§ ä½¿ç”¨ä¸å¹³è¡¡å¤„ç†æ–¹æ³•(ROS): {method}")

        y = np.asarray(y)
        classes, counts = np.unique(y, return_counts=True)
        max_n = counts.max()
        rng = np.random.default_rng(42)
        indices = []
        for c in classes:
            idx = np.where(y == c)[0]
            if len(idx) == 0:
                continue
            if len(idx) < max_n:
                extra = rng.choice(idx, size=max_n - len(idx), replace=True)
                idx = np.concatenate([idx, extra], axis=0)
            indices.append(idx)

        sel = np.concatenate(indices, axis=0)
        print(f"âœ“ é‡‡æ ·å®Œæˆï¼Œæ ·æœ¬æ•°: {len(X)} -> {len(sel)}")
        return [X[i] for i in sel], y[sel]
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        info = {
            "device": str(self.device),
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "label_encoder_loaded": self.label_encoder is not None,
        }
        
        if self.label_encoder is not None:
            info["num_classes"] = len(self.label_encoder.classes_)
            info["classes"] = list(self.label_encoder.classes_)
        
        if self.model is not None:
            info["model_type"] = type(self.model).__name__
            
        return info