#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BMW Case-Item é¡¹ç›®è¯„ä¼°å·¥å…·
æ”¯æŒå®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»ï¼ŒåŒ…æ‹¬æ¨¡å‹è´¨é‡ã€ç»“æ„æ€§åˆ†æå’Œå·¥ç¨‹æ€§èƒ½
"""

import os
import json
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import time
import warnings

warnings.filterwarnings("ignore")

# å¯¼å…¥æˆ‘ä»¬çš„ç®¡ç†ç³»ç»Ÿ
from model_manager import ModelManager
from config_manager import get_config_manager
from error_handler import get_error_handler, log_info, log_warning, log_error
from utils import ensure_single_label, build_text, _flex_read_csv


def calculate_hit_at_k(predictions: List[List[str]], ground_truth: List[str], k: int) -> float:
    """è®¡ç®— Hit@K"""
    if len(predictions) != len(ground_truth):
        raise ValueError("é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾æ•°é‡ä¸åŒ¹é…")
    
    hits = 0
    for pred, true in zip(predictions, ground_truth):
        if true in pred[:k]:
            hits += 1
    return hits / len(predictions)


def calculate_mrr(predictions: List[List[str]], ground_truth: List[str]) -> float:
    """è®¡ç®— MRRï¼ˆå¹³å‡å€’æ•°æ’åï¼‰"""
    if len(predictions) != len(ground_truth):
        raise ValueError("é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾æ•°é‡ä¸åŒ¹é…")
    
    reciprocal_ranks = []
    for pred, true in zip(predictions, ground_truth):
        try:
            rank = pred.index(true) + 1
            reciprocal_ranks.append(1.0 / rank)
        except ValueError:
            reciprocal_ranks.append(0.0)
    return sum(reciprocal_ranks) / len(reciprocal_ranks)


def calculate_ndcg_at_k(predictions: List[List[str]], ground_truth: List[str], 
                    relevance_scores: Optional[Dict[str, float]], k: int) -> float:
    """è®¡ç®— NDCG@K"""
    if len(predictions) != len(ground_truth):
        raise ValueError("é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾æ•°é‡ä¸åŒ¹é…")
    
    # å¦‚æœæ²¡æœ‰æä¾›ç›¸å…³æ€§åˆ†æ•°ï¼Œå‡è®¾æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆç›¸å…³æ€§ä¸º1
    if relevance_scores is None:
        relevance_scores = {item: 1.0 for item in set(ground_truth)}
    
    dcg = 0.0
    idcg = 0.0
    
    # è®¡ç®— IDCGï¼ˆç†æƒ³DCGï¼‰
    sorted_relevance = sorted([relevance_scores.get(item, 0.0) for item in set(ground_truth)], reverse=True)
    idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(sorted_relevance[:k]))
    
    if idcg == 0:
        return 0.0
    
    # è®¡ç®— DCG
    for pred, true in zip(predictions, ground_truth):
        for i, item in enumerate(pred[:k]):
            if item == true:
                dcg += relevance_scores.get(item, 0.0) / np.log2(i + 2)
                break
    
    return dcg / idcg


def calculate_coverage(all_items: List[str], predicted_items: List[List[str]], top_k: int) -> float:
    """è®¡ç®—è¦†ç›–ç‡"""
    predicted_set = set()
    for pred in predicted_items:
        predicted_set.update(pred[:top_k])
    
    return len(predicted_set & set(all_items)) / len(set(all_items))


def analyze_corrected_cases(base_predictions: List[List[str]], rerank_predictions: List[List[str]], 
                           ground_truth: List[str], k: int = 3) -> Dict:
    """åˆ†æçº é”™æ¡ˆä¾‹"""
    corrected = 0
    misranked = 0
    corrected_cases = []
    misranked_cases = []
    
    for i, (base_pred, rerank_pred, true) in enumerate(zip(base_predictions, rerank_predictions, ground_truth)):
        base_hit = true in base_pred[:k]
        rerank_hit = true in rerank_pred[:k]
        
        if not base_hit and rerank_hit:
            corrected += 1
            corrected_cases.append({
                'index': i,
                'true_label': true,
                'base_prediction': base_pred[:k],
                'rerank_prediction': rerank_pred[:k],
                'base_rank': base_pred.index(true) + 1 if true in base_pred else None,
                'rerank_rank': rerank_pred.index(true) + 1 if true in rerank_pred else None
            })
        elif base_hit and not rerank_hit:
            misranked += 1
            misranked_cases.append({
                'index': i,
                'true_label': true,
                'base_prediction': base_pred[:k],
                'rerank_prediction': rerank_pred[:k],
                'base_rank': base_pred.index(true) + 1,
                'rerank_rank': rerank_pred.index(true) + 1
            })
    
    return {
        'corrected': corrected,
        'misranked': misranked,
        'corrected_rate': corrected / len(ground_truth),
        'misranked_rate': misranked / len(ground_truth),
        'corrected_cases': corrected_cases,
        'misranked_cases': misranked_cases
    }


def analyze_error_types(predictions: List[str], ground_truth: List[str], 
                   texts: List[str], k: int = 3) -> Dict:
    """åˆ†æé”™è¯¯ç±»å‹"""
    error_types = {
        'text_noise': 0,      # æ–‡æœ¬å™ªå£°ï¼ˆæ‹¼å†™é”™è¯¯ã€å¼‚å¸¸å­—æ®µï¼‰
        'vague_description': 0,  # item æè¿°æ¨¡ç³Š
        'too_short': 0,        # case è¿‡çŸ­
        'too_long': 0,         # case è¿‡é•¿
        'template_mismatch': 0, # æ¨¡æ¿åŒ– case â†’ æŸç±» item è¢«å›ºå®šé”™è¯¯é¢„æµ‹
        'ambiguous': 0,         # å¤šä¹‰ case â†’ æ··æ·†å¤šç±»
        'other': 0              # å…¶ä»–é”™è¯¯
    }
    
    error_details = []
    
    for i, (pred, true, text) in enumerate(zip(predictions, ground_truth, texts)):
        if true not in pred[:k]:
            # ç®€å•çš„é”™è¯¯ç±»å‹åˆ†æï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
            text_len = len(text)
            
            if text_len < 10:
                error_types['too_short'] += 1
                error_type = 'too_short'
            elif text_len > 500:
                error_types['too_long'] += 1
                error_type = 'too_long'
            elif any(char.isdigit() for char in text) and any(char.isalpha() for char in text):
                error_types['text_noise'] += 1
                error_type = 'text_noise'
            else:
                error_types['other'] += 1
                error_type = 'other'
            
            error_details.append({
                'index': i,
                'true_label': true,
                'predicted_label': pred[0],
                'text': text[:100] + '...' if len(text) > 100 else text,
                'error_type': error_type
            })
    
    total_errors = sum(error_types.values())
    error_percentages = {k: v/total_errors for k, v in error_types.items() if total_errors > 0}
    
    return {
        'error_counts': error_types,
        'error_percentages': error_percentages,
        'total_errors': total_errors,
        'error_details': error_details
    }


def plot_confusion_matrix(cm, class_names, output_path: str):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(15, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix', fontsize=16)
    plt.ylabel('True Label', fontsize=14)
    plt.xlabel('Predicted Label', fontsize=14)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()


def plot_metric_comparison(metrics_df, output_path: str):
    """ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Hit@K å¯¹æ¯”
    metrics_df[['Hit@1', 'Hit@3', 'Hit@10']].plot(kind='bar', ax=axes[0, 0])
    axes[0, 0].set_title('Hit@K Comparison')
    axes[0, 0].set_ylabel('Hit Rate')
    
    # MRR å’Œ NDCG å¯¹æ¯”
    metrics_df[['MRR', 'NDCG@3', 'NDCG@10']].plot(kind='bar', ax=axes[0, 1])
    axes[0, 1].set_title('Ranking Quality Comparison')
    axes[0, 1].set_ylabel('Score')
    
    # Coverage å’Œå…¶ä»–æŒ‡æ ‡
    other_cols = [col for col in metrics_df.columns if col not in ['Hit@1', 'Hit@3', 'Hit@10', 'MRR', 'NDCG@3', 'NDCG@10']]
    if other_cols:
        metrics_df[other_cols].plot(kind='bar', ax=axes[0, 2])
        axes[0, 2].set_title('Other Metrics')
        axes[0, 2].set_ylabel('Value')
    
    # å·¥ç¨‹æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'Latency' in metrics_df.columns:
        metrics_df[['Latency']].plot(kind='bar', ax=axes[1, 0])
        axes[1, 0].set_title('Latency Comparison')
        axes[1, 0].set_ylabel('Latency (ms)')
    
    if 'TP99' in metrics_df.columns:
        metrics_df[['TP99']].plot(kind='bar', ax=axes[1, 1])
        axes[1, 1].set_title('TP99 Latency')
        axes[1, 1].set_ylabel('TP99 (ms)')
    
    # TPSï¼ˆå¦‚æœæœ‰ï¼‰
    if 'TPS' in metrics_df.columns:
        metrics_df[['TPS']].plot(kind='bar', ax=axes[1, 2])
        axes[1, 2].set_title('Throughput')
        axes[1, 2].set_ylabel('TPS')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()


def evaluate_model(predictions_file: str, ground_truth_file: str, 
                model_name: str, output_dir: str) -> Dict:
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    log_info(f"ğŸ” å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
    
    # è¯»å–é¢„æµ‹ç»“æœ
    if predictions_file.endswith('.json'):
        with open(predictions_file, 'r', encoding='utf-8') as f:
            predictions_data = json.load(f)
        
        predictions = [item['top_10_predictions'] if 'top_10_predictions' in item else [item['predicted_label']] 
                     for item in predictions_data]
        confidences = [item['confidence'] for item in predictions_data]
    else:
        pred_df = pd.read_csv(predictions_file)
        predictions = [pred_df['top_10_predictions'].iloc[i] if 'top_10_predictions' in pred_df.columns 
                     else [pred_df['predicted_label'].iloc[i]] for i in range(len(pred_df))]
        confidences = pred_df['confidence'].tolist() if 'confidence' in pred_df.columns else [1.0] * len(pred_df)
    
    # è¯»å–çœŸå®æ ‡ç­¾
    truth_df = pd.read_csv(ground_truth_file)
    ground_truth = truth_df['linked_items'].astype(str).tolist()
    
    # è¯»å–æ–‡æœ¬ï¼ˆç”¨äºé”™è¯¯åˆ†æï¼‰
    texts = []
    if 'case_title' in truth_df.columns and 'performed_work' in truth_df.columns:
        texts = (truth_df['case_title'] + ' ' + truth_df['performed_work']).tolist()
    
    # ç¡®ä¿æ•°æ®é‡ä¸€è‡´
    min_len = min(len(predictions), len(ground_truth))
    predictions = predictions[:min_len]
    ground_truth = ground_truth[:min_len]
    texts = texts[:min_len]
    
    # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    hit_1 = calculate_hit_at_k(predictions, ground_truth, 1)
    hit_3 = calculate_hit_at_k(predictions, ground_truth, 3)
    hit_10 = calculate_hit_at_k(predictions, ground_truth, 10)
    mrr = calculate_mrr(predictions, ground_truth)
    ndcg_3 = calculate_ndcg_at_k(predictions, ground_truth, None, 3)
    ndcg_10 = calculate_ndcg_at_k(predictions, ground_truth, None, 10)
    
    # è®¡ç®—è¦†ç›–ç‡
    all_items = list(set(ground_truth))
    coverage_3 = calculate_coverage(all_items, predictions, 3)
    coverage_10 = calculate_coverage(all_items, predictions, 10)
    
    # åˆ†æé”™è¯¯ç±»å‹
    error_analysis = analyze_error_types(predictions, ground_truth, texts, 3)
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    le = LabelEncoder()
    all_labels = list(set(ground_truth + [pred[0] for pred in predictions]))
    le.fit(all_labels)
    
    y_true_encoded = le.transform(ground_truth)
    y_pred_encoded = le.transform([pred[0] for pred in predictions])
    
    cm = confusion_matrix(y_true_encoded, y_pred_encoded)
    
    results = {
        'model_name': model_name,
        'total_samples': len(predictions),
        'hit_1': hit_1,
        'hit_3': hit_3,
        'hit_10': hit_10,
        'mrr': mrr,
        'ndcg_3': ndcg_3,
        'ndcg_10': ndcg_10,
        'coverage_3': coverage_3,
        'coverage_10': coverage_10,
        'error_analysis': error_analysis,
        'confusion_matrix': cm.tolist(),
        'class_names': le.classes_.tolist()
    }
    
    # ä¿å­˜ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ•°å€¼ç»“æœ
    results_df = pd.DataFrame([{
        'Model': model_name,
        'Hit@1': hit_1,
        'Hit@3': hit_3,
        'Hit@10': hit_10,
        'MRR': mrr,
        'NDCG@3': ndcg_3,
        'NDCG@10': ndcg_10,
        'Coverage@3': coverage_3,
        'Coverage@10': coverage_10,
        'Total Errors': error_analysis['total_errors'],
        'Error Rate': error_analysis['total_errors'] / len(predictions)
    }])
    
    results_path = os.path.join(output_dir, f"{model_name}_metrics.csv")
    results_df.to_csv(results_path, index=False)
    log_info(f"âœ“ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {results_path}")
    
    # ä¿å­˜é”™è¯¯è¯¦æƒ…
    if error_analysis['error_details']:
        error_details_df = pd.DataFrame(error_analysis['error_details'])
        error_details_path = os.path.join(output_dir, f"{model_name}_error_details.csv")
        error_details_df.to_csv(error_details_path, index=False)
        log_info(f"âœ“ é”™è¯¯è¯¦æƒ…å·²ä¿å­˜åˆ°: {error_details_path}")
    
    # ä¿å­˜æ··æ·†çŸ©é˜µ
    cm_path = os.path.join(output_dir, f"{model_name}_confusion_matrix.png")
    plot_confusion_matrix(cm, le.classes_, cm_path)
    log_info(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_path}")
    
    return results


def compare_models(base_predictions: str, rerank_predictions: str, 
                 ground_truth: str, output_dir: str) -> Dict:
    """æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’Œrerankæ¨¡å‹"""
    log_info("ğŸ”„ å¼€å§‹æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’Œrerankæ¨¡å‹")
    
    # è¯»å–é¢„æµ‹ç»“æœ
    with open(base_predictions, 'r', encoding='utf-8') as f:
        base_data = json.load(f)
    
    with open(rerank_predictions, 'r', encoding='utf-8') as f:
        rerank_data = json.load(f)
    
    # è¯»å–çœŸå®æ ‡ç­¾
    truth_df = pd.read_csv(ground_truth)
    ground_truth = truth_df['linked_items'].astype(str).tolist()
    
    # æå–é¢„æµ‹åˆ—è¡¨
    base_preds = [item['top_10_predictions'] if 'top_10_predictions' in item else [item['predicted_label']] 
                  for item in base_data]
    rerank_preds = [item['top_10_predictions'] if 'top_10_predictions' in item else [item['predicted_label']] 
                    for item in rerank_data]
    
    # ç¡®ä¿æ•°æ®é‡ä¸€è‡´
    min_len = min(len(base_preds), len(rerank_preds), len(ground_truth))
    base_preds = base_preds[:min_len]
    rerank_preds = rerank_preds[:min_len]
    ground_truth = ground_truth[:min_len]
    
    # åˆ†æçº é”™æ¡ˆä¾‹
    corrected_analysis = analyze_corrected_cases(base_preds, rerank_preds, ground_truth, 3)
    
    # è®¡ç®—å„è‡ªæŒ‡æ ‡
    base_hit_3 = calculate_hit_at_k(base_preds, ground_truth, 3)
    rerank_hit_3 = calculate_hit_at_k(rerank_preds, ground_truth, 3)
    
    base_mrr = calculate_mrr(base_preds, ground_truth)
    rerank_mrr = calculate_mrr(rerank_preds, ground_truth)
    
    # ä¿å­˜ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    
    comparison_results = {
        'base_hit_3': base_hit_3,
        'rerank_hit_3': rerank_hit_3,
        'delta_hit_3': rerank_hit_3 - base_hit_3,
        'base_mrr': base_mrr,
        'rerank_mrr': rerank_mrr,
        'delta_mrr': rerank_mrr - base_mrr,
        'corrected_cases': corrected_analysis['corrected'],
        'misranked_cases': corrected_analysis['misranked'],
        'corrected_rate': corrected_analysis['corrected_rate'],
        'misranked_rate': corrected_analysis['misranked_rate']
    }
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    comparison_df = pd.DataFrame([comparison_results])
    comparison_path = os.path.join(output_dir, "rerank_comparison.csv")
    comparison_df.to_csv(comparison_path, index=False)
    log_info(f"âœ“ Rerankæ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: {comparison_path}")
    
    # ä¿å­˜çº é”™æ¡ˆä¾‹
    if corrected_analysis['corrected_cases']:
        corrected_df = pd.DataFrame(corrected_analysis['corrected_cases'])
        corrected_path = os.path.join(output_dir, "corrected_cases.csv")
        corrected_df.to_csv(corrected_path, index=False)
        log_info(f"âœ“ çº é”™æ¡ˆä¾‹å·²ä¿å­˜åˆ°: {corrected_path}")
    
    if corrected_analysis['misranked_cases']:
        misranked_df = pd.DataFrame(corrected_analysis['misranked_cases'])
        misranked_path = os.path.join(output_dir, "misranked_cases.csv")
        misranked_df.to_csv(misranked_path, index=False)
        log_info(f"âœ“ é”™æ’æ¡ˆä¾‹å·²ä¿å­˜åˆ°: {misranked_path}")
    
    return comparison_results


def generate_evaluation_report(tfidf_results: Optional[str], bert_results: Optional[str], 
                          rerank_results: Optional[str], output_dir: str):
    """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
    log_info("ğŸ“Š ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š")
    
    all_results = []
    
    if tfidf_results and os.path.exists(tfidf_results):
        tfidf_df = pd.read_csv(tfidf_results)
        all_results.append(tfidf_df)
    
    if bert_results and os.path.exists(bert_results):
        bert_df = pd.read_csv(bert_results)
        all_results.append(bert_df)
    
    if rerank_results and os.path.exists(rerank_results):
        rerank_df = pd.read_csv(rerank_results)
        all_results.append(rerank_df)
    
    if not all_results:
        log_warning("æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    # åˆå¹¶ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    plot_path = os.path.join(output_dir, "metrics_comparison.png")
    plot_metric_comparison(combined_df, plot_path)
    log_info(f"âœ“ æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {plot_path}")
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>BMW Case-Item è¯„ä¼°æŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
            .section {{ margin-bottom: 30px; }}
            .metric-table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
            .metric-table th, .metric-table td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}
            .metric-table th {{ background-color: #f2f2f2; }}
            .improvement {{ color: green; }}
            .degradation {{ color: red; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸ“˜ BMW Case-Item é¡¹ç›®è¯„ä¼°æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="section">
            <h2>ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”</h2>
            <table class="metric-table">
                <tr>
                    <th>æ¨¡å‹</th>
                    <th>Hit@1</th>
                    <th>Hit@3</th>
                    <th>Hit@10</th>
                    <th>MRR</th>
                    <th>NDCG@3</th>
                    <th>NDCG@10</th>
                    <th>Coverage@3</th>
                    <th>Coverage@10</th>
                </tr>
    """
    
    for _, row in combined_df.iterrows():
        html_content += f"""
                <tr>
                    <td>{row['Model']}</td>
                    <td>{row['Hit@1']:.4f}</td>
                    <td>{row['Hit@3']:.4f}</td>
                    <td>{row['Hit@10']:.4f}</td>
                    <td>{row['MRR']:.4f}</td>
                    <td>{row['NDCG@3']:.4f}</td>
                    <td>{row['NDCG@10']:.4f}</td>
                    <td>{row['Coverage@3']:.4f}</td>
                    <td>{row['Coverage@10']:.4f}</td>
                </tr>
        """
    
    html_content += """
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ“ˆ æŒ‡æ ‡å¯¹æ¯”å›¾</h2>
            <img src="metrics_comparison.png" alt="æŒ‡æ ‡å¯¹æ¯”å›¾" style="max-width: 100%;">
        </div>
        
        <div class="section">
            <h2>ğŸ’¡ æ”¹è¿›å»ºè®®</h2>
            <ul>
                <li>é‡ç‚¹å…³æ³¨ Hit@3 æŒ‡æ ‡ï¼Œè¿™æ˜¯ä¸šåŠ¡æ ¸å¿ƒæŒ‡æ ‡</li>
                <li>åˆ†æé«˜é¢‘é”™è¯¯æ¨¡å¼ï¼Œé’ˆå¯¹æ€§æ”¹è¿›æ•°æ®é¢„å¤„ç†</li>
                <li>è€ƒè™‘ Rerank ç­–ç•¥ä¼˜åŒ–ï¼Œå‡å°‘ Mis-rank Cases</li>
                <li>ç›‘æ§ Coverage æŒ‡æ ‡ï¼Œç¡®ä¿å†·é—¨ Item æœ‰è¶³å¤Ÿæ›å…‰</li>
            </ul>
        </div>
    </body>
    </html>
    """
    
    report_path = os.path.join(output_dir, "evaluation_report.html")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    log_info(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="BMW Case-Item é¡¹ç›®è¯„ä¼°å·¥å…·")
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è¯„ä¼°å•ä¸ªæ¨¡å‹
    eval_parser = subparsers.add_parser('evaluate', help='è¯„ä¼°å•ä¸ªæ¨¡å‹')
    eval_parser.add_argument('--predictions', required=True, help='é¢„æµ‹ç»“æœæ–‡ä»¶ï¼ˆCSVæˆ–JSONï¼‰')
    eval_parser.add_argument('--ground-truth', required=True, help='çœŸå®æ ‡ç­¾æ–‡ä»¶ï¼ˆCSVï¼‰')
    eval_parser.add_argument('--model-name', required=True, help='æ¨¡å‹åç§°')
    eval_parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    
    # æ¯”è¾ƒæ¨¡å‹
    compare_parser = subparsers.add_parser('compare', help='æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’Œrerankæ¨¡å‹')
    compare_parser.add_argument('--base-predictions', required=True, help='åŸºç¡€æ¨¡å‹é¢„æµ‹ç»“æœï¼ˆJSONï¼‰')
    compare_parser.add_argument('--rerank-predictions', required=True, help='Rerankæ¨¡å‹é¢„æµ‹ç»“æœï¼ˆJSONï¼‰')
    compare_parser.add_argument('--ground-truth', required=True, help='çœŸå®æ ‡ç­¾æ–‡ä»¶ï¼ˆCSVï¼‰')
    compare_parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    
    # ç”ŸæˆæŠ¥å‘Š
    report_parser = subparsers.add_parser('report', help='ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š')
    report_parser.add_argument('--tfidf-results', help='TF-IDFè¯„ä¼°ç»“æœï¼ˆCSVï¼‰')
    report_parser.add_argument('--bert-results', help='BERTè¯„ä¼°ç»“æœï¼ˆCSVï¼‰')
    report_parser.add_argument('--rerank-results', help='Rerankè¯„ä¼°ç»“æœï¼ˆCSVï¼‰')
    report_parser.add_argument('--output-dir', required=True, help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    error_handler = get_error_handler(
        log_file=f"./logs/evaluation_{time.strftime('%Y%m%d_%H%M%S')}.log"
    )
    
    if args.command == 'evaluate':
        results = evaluate_model(
            args.predictions, args.ground_truth, args.model_name, args.output_dir
        )
        log_info(f"âœ“ æ¨¡å‹ {args.model_name} è¯„ä¼°å®Œæˆ")
        
    elif args.command == 'compare':
        results = compare_models(
            args.base_predictions, args.rerank_predictions, args.ground_truth, args.output_dir
        )
        log_info("âœ“ æ¨¡å‹æ¯”è¾ƒå®Œæˆ")
        
    elif args.command == 'report':
        generate_evaluation_report(
            args.tfidf_results, args.bert_results, args.rerank_results, args.output_dir
        )
        log_info("âœ“ è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
    else:
        parser.print_help()


if __name__ == "__main__":
    main()